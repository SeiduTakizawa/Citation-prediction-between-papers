{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:52:25.070270Z",
     "iopub.status.busy": "2025-05-30T19:52:25.069907Z",
     "iopub.status.idle": "2025-05-30T19:52:33.322071Z",
     "shell.execute_reply": "2025-05-30T19:52:33.321237Z",
     "shell.execute_reply.started": "2025-05-30T19:52:25.070244Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Config paths\n",
    "# ----------------------------\n",
    "EDGES_POS = '/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt'\n",
    "TEST_TXT  = '/kaggle/input/nlp-cse-uoi-2025/data_new/test.txt'\n",
    "OUT_PKL   = 'graph_features_test.pkl'\n",
    "OUT_CSV   = 'graph_features_test.csv'\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load positive edges and build graph\n",
    "# ----------------------------\n",
    "edges_pos = pd.read_csv(\n",
    "    EDGES_POS,\n",
    "    header=None,\n",
    "    names=['u', 'v'],\n",
    "    dtype={'u': int, 'v': int}\n",
    ")\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges_pos.values)\n",
    "# Ensure isolated nodes are included (optional)\n",
    "all_nodes = set(edges_pos.u) | set(edges_pos.v)\n",
    "G.add_nodes_from(all_nodes)\n",
    "print(f\"Graph built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load test pairs\n",
    "# ----------------------------\n",
    "def load_test_pairs(fp: str) -> list:\n",
    "    pairs = []\n",
    "    with open(fp, encoding='utf-8') as fh:\n",
    "        for ln in fh:\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            try:\n",
    "                u, v = map(int, ln.split(',')[:2])\n",
    "                pairs.append((u, v))\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return pairs\n",
    "\n",
    "test_pairs = load_test_pairs(TEST_TXT)\n",
    "print(f\"Loaded {len(test_pairs):,} test pairs.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Define graph-based feature functions\n",
    "# ----------------------------\n",
    "def graph_features(u: int, v: int) -> np.ndarray:\n",
    "    # Common neighbors (log1p scaled)\n",
    "    try:\n",
    "        cn = np.log1p(len(list(nx.common_neighbors(G, u, v))))\n",
    "    except Exception:\n",
    "        cn = 0.0\n",
    "    # Preferential attachment (log1p scaled)\n",
    "    try:\n",
    "        pa_scores = list(nx.preferential_attachment(G, [(u, v)]))\n",
    "        raw_pa = pa_scores[0][2] if pa_scores else 0\n",
    "        pa = np.log1p(raw_pa)\n",
    "    except Exception:\n",
    "        pa = 0.0\n",
    "    return np.array([cn, pa], dtype=np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Compute features for all test pairs\n",
    "# ----------------------------\n",
    "graph_feat_dict = {}\n",
    "rows = []\n",
    "for idx, (u, v) in enumerate(tqdm(test_pairs, desc='Computing graph features')):\n",
    "    feat = graph_features(u, v)\n",
    "    graph_feat_dict[(u, v)] = feat\n",
    "    rows.append({'ID': idx, 'u': u, 'v': v, 'cn': float(feat[0]), 'pa': float(feat[1])})\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Save outputs\n",
    "# ----------------------------\n",
    "# Pickle for code lookup\n",
    "with open(OUT_PKL, 'wb') as f:\n",
    "    pickle.dump(graph_feat_dict, f)\n",
    "print(f\"Saved pickle -> {OUT_PKL}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:14:31.832170Z",
     "iopub.status.busy": "2025-05-30T19:14:31.831880Z",
     "iopub.status.idle": "2025-05-30T19:50:37.898291Z",
     "shell.execute_reply": "2025-05-30T19:50:37.897413Z",
     "shell.execute_reply.started": "2025-05-30T19:14:31.832149Z"
    }
   },
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION MODEL\n",
    "\n",
    "\n",
    "# 1. Load precomputed raw embeddings and features\n",
    "\n",
    "with open(\"/kaggle/input/scibert/scibert_embeddings.pkl\", \"rb\") as f:\n",
    "    bert_emb_raw = pickle.load(f)\n",
    "with open(\"/kaggle/input/n2v-rich/node2vec_embeddings_train (1).pkl\", \"rb\") as f:\n",
    "    n2v_emb_raw = pickle.load(f)\n",
    "with open(\"/kaggle/input/fine-tuned-embed/cls_embeddings (3).npy\", \"rb\") as f:\n",
    "    cls_emb_raw = np.load(f)\n",
    "with open(\"/kaggle/input/graph-feats/graph_features (4).pkl\", \"rb\") as f:\n",
    "    graph_feats = pickle.load(f)\n",
    "\n",
    "\n",
    "# 2. Load abstracts and edges\n",
    "\n",
    "abstracts = []\n",
    "with open(\"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split(\"|--|\")\n",
    "        abstracts.append(parts[1].strip().lower() if len(parts)==2 else line.strip().lower())\n",
    "\n",
    "edges_pos = pd.read_csv(\n",
    "    \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\", header=None,\n",
    "    names=[\"u\", \"v\"], dtype=int\n",
    ")\n",
    "edges_pos[\"y\"] = 1\n",
    "\n",
    "\n",
    "# 3. Load authors\n",
    "\n",
    "def load_authors(fp: str) -> dict:\n",
    "    d = {}\n",
    "    with open(fp, encoding=\"utf-8\") as fh:\n",
    "        for ln in fh:\n",
    "            if \"|--|\" not in ln: continue\n",
    "            pid_str, names = ln.rstrip().split(\"|--|\",1)\n",
    "            try:\n",
    "                pid = int(pid_str)\n",
    "                d[pid] = {n.strip() for n in names.split(\",\") if n.strip()}\n",
    "            except:\n",
    "                pass\n",
    "    return d\n",
    "\n",
    "paper_auth = load_authors(\"/kaggle/input/nlp-cse-uoi-2025/data_new/authors.txt\")\n",
    "\n",
    "\n",
    "# 4. Sample negatives and split\n",
    "\n",
    "def sample_negatives(df, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_set = set(map(tuple, df[[\"u\",\"v\"]].values))\n",
    "    N = df[[\"u\",\"v\"]].values.max()+1\n",
    "    neg = set()\n",
    "    while len(neg) < len(df):\n",
    "        u,v = rng.integers(0, N, size=2)\n",
    "        if u!=v and (u,v) not in pos_set and (v,u) not in pos_set:\n",
    "            neg.add((u,v))\n",
    "    return pd.DataFrame(list(neg), columns=[\"u\",\"v\"]), N\n",
    "\n",
    "edges_neg, num_nodes = sample_negatives(edges_pos)\n",
    "edges_neg[\"y\"] = 0\n",
    "full = pd.concat([edges_pos, edges_neg], ignore_index=True)\n",
    "train_df, test_df = train_test_split(full, test_size=0.2, stratify=full.y, random_state=42)\n",
    "\n",
    "\n",
    "# 5. Fit PCA on train nodes\n",
    "\n",
    "train_nodes = pd.unique(train_df[[\"u\",\"v\"]].values.ravel())\n",
    "bert_mat = np.stack([bert_emb_raw[n] for n in train_nodes], axis=0)\n",
    "n2v_mat  = np.stack([n2v_emb_raw[n] for n in train_nodes], axis=0)\n",
    "cls_mat  = np.stack([cls_emb_raw[n] for n in train_nodes], axis=0)\n",
    "\n",
    "pca_bert = PCA(n_components=55, random_state=42).fit(bert_mat)\n",
    "pca_n2v  = PCA(n_components=108, random_state=42).fit(n2v_mat)\n",
    "pca_cls  = PCA(n_components=32, random_state=42).fit(cls_mat)\n",
    "\n",
    "# Transform raw\n",
    "bert_emb = {n: pca_bert.transform(bert_emb_raw[n].reshape(1,-1))[0] for n in bert_emb_raw}\n",
    "n2v_emb  = {n: pca_n2v.transform(n2v_emb_raw[n].reshape(1,-1))[0]  for n in n2v_emb_raw}\n",
    "cls_emb  = {n: pca_cls.transform(cls_emb_raw[n].reshape(1,-1))[0]      for n in range(len(cls_emb_raw))}\n",
    "\n",
    "\n",
    "# 6. Fit TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', min_df=20)\n",
    "train_texts = [abstracts[n] for n in train_nodes]\n",
    "vectorizer.fit(train_texts)\n",
    "tfidf_matrix = vectorizer.transform(abstracts)\n",
    "\n",
    "\n",
    "# 7. Feature builder\n",
    "\n",
    "def tfidf_cosine(u,v):\n",
    "    try:\n",
    "        return float(cosine_similarity(tfidf_matrix[u], tfidf_matrix[v])[0,0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def author_feat_jaccard(u, v):\n",
    "    Au = paper_auth.get(u, set())\n",
    "    Av = paper_auth.get(v, set())\n",
    "    inter = len(Au & Av)\n",
    "    union = len(Au | Av)\n",
    "    return 0.0 if union == 0 else inter / union\n",
    "\n",
    "\n",
    "\n",
    "def build_features(df):\n",
    "    X = []\n",
    "    for u, v in tqdm(df[['u','v']].itertuples(index=False), total=len(df), desc=\"Building features\"):\n",
    "        # concatenate all embedding vectors into a dense one\n",
    "        eu = np.concatenate([n2v_emb[u], cls_emb[u],bert_emb[u]])\n",
    "        ev = np.concatenate([n2v_emb[v], cls_emb[v],bert_emb[v]])\n",
    "        \n",
    "        # difference & elementwise product\n",
    "        diff = np.abs(eu - ev)\n",
    "        prod = eu * ev\n",
    "        \n",
    "        # cosine similarity of the two combined embeddings\n",
    "        sim = float(np.dot(eu, ev) /\n",
    "                    (np.linalg.norm(eu) * np.linalg.norm(ev) + 1e-9))\n",
    "        \n",
    "        # recomputed graph-based features\n",
    "        g = graph_feats.get((u, v),\n",
    "                            graph_feats.get((v, u),\n",
    "                                            np.zeros(2, dtype=np.float32)))\n",
    "        \n",
    "        # additional hand‐crafted similarities\n",
    "        aj = author_feat_jaccard(u, v)\n",
    "        tc = tfidf_cosine(u, v)\n",
    "        meta = np.array([aj, tc, sim], dtype=np.float32)\n",
    "        \n",
    "        # final concatenation\n",
    "        X.append(np.concatenate([eu, ev, diff, prod, g, meta]))\n",
    "    \n",
    "    return np.stack(X)\n",
    "\n",
    "\n",
    "\n",
    "# 8. Train & eval\n",
    "\n",
    "X_train = build_features(train_df)\n",
    "X_test  = build_features(test_df)\n",
    "y_train,y_test = train_df.y.values, test_df.y.values\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "clf = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=5000, n_jobs=-1) #jobs=-1 to use all of the kaggle cores.\n",
    "clf.fit(scaler.transform(X_train), y_train)\n",
    "probs = clf.predict_proba(scaler.transform(X_test))[:,1]\n",
    "preds = (probs>0.5).astype(int)\n",
    "print('Log-loss:', log_loss(y_test, probs))\n",
    "print('AUROC   :', roc_auc_score(y_test, probs))\n",
    "print('F1      :', f1_score(y_test, preds))\n",
    "\n",
    "\n",
    "# 9. Save artifacts for more future testing\n",
    "\n",
    "with open('pca_bert.pkl','wb') as f: pickle.dump(pca_bert,f)\n",
    "with open('pca_n2v.pkl','wb')  as f: pickle.dump(pca_n2v,f)\n",
    "with open('pca_cls.pkl','wb')  as f: pickle.dump(pca_cls,f)\n",
    "with open('tfidf_vect.pkl','wb') as f: pickle.dump(vectorizer,f)\n",
    "with open('scaler.pkl','wb')    as f: pickle.dump(scaler,f)\n",
    "with open('logreg_model.pkl','wb') as f: pickle.dump(clf,f)\n",
    "print('✅ Saved all artifacts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:56:04.841568Z",
     "iopub.status.busy": "2025-05-30T19:56:04.841249Z",
     "iopub.status.idle": "2025-05-30T19:58:45.952992Z",
     "shell.execute_reply": "2025-05-30T19:58:45.952033Z",
     "shell.execute_reply.started": "2025-05-30T19:56:04.841545Z"
    }
   },
   "outputs": [],
   "source": [
    "#TESTING OUR OWN PAIRS \n",
    "\n",
    "\n",
    "\n",
    "# Config & paths\n",
    "\n",
    "EDGES_POS       = '/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt'\n",
    "TEST_TXT        = '/kaggle/input/nlp-cse-uoi-2025/data_new/test.txt'\n",
    "ABSTRACTS_TXT   = '/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt'\n",
    "AUTHORS_PATH    = '/kaggle/input/nlp-cse-uoi-2025/data_new/authors.txt'\n",
    "\n",
    "PCA_BERT_PATH   = 'pca_bert.pkl'\n",
    "PCA_N2V_PATH    = 'pca_n2v.pkl'\n",
    "PCA_CLS_PATH    = 'pca_cls.pkl'\n",
    "TFIDF_VECT_PATH = 'tfidf_vect.pkl'\n",
    "SCALER_PATH     = 'scaler.pkl'\n",
    "MODEL_PATH      = 'logreg_model.pkl'\n",
    "GRAPH_FEAT_PATH = 'graph_features_test.pkl'\n",
    "\n",
    "OUTPUT_SUB_PATH = 'submission.csv'\n",
    "\n",
    "# 1. Load artifacts\n",
    "\n",
    "with open(PCA_BERT_PATH,   'rb') as f: pca_bert    = pickle.load(f)\n",
    "with open(PCA_N2V_PATH,    'rb') as f: pca_n2v     = pickle.load(f)\n",
    "with open(PCA_CLS_PATH,    'rb') as f: pca_cls     = pickle.load(f)\n",
    "with open(TFIDF_VECT_PATH, 'rb') as f: tfidf_vect  = pickle.load(f)\n",
    "with open(SCALER_PATH,     'rb') as f: scaler      = pickle.load(f)\n",
    "with open(MODEL_PATH,      'rb') as f: model       = pickle.load(f)\n",
    "with open(GRAPH_FEAT_PATH, 'rb') as f: graph_feats = pickle.load(f)\n",
    "\n",
    "# Raw embeddings\n",
    "with open('/kaggle/input/scibert/scibert_embeddings.pkl','rb') as f:\n",
    "    bert_raw = pickle.load(f)\n",
    "with open('/kaggle/input/n2v-rich/node2vec_embeddings_train (1).pkl','rb') as f:\n",
    "    n2v_raw  = pickle.load(f)\n",
    "cls_raw = np.load('/kaggle/input/fine-tuned-embed/cls_embeddings (3).npy')\n",
    "\n",
    "\n",
    "# 2. Load abstracts & build TF-IDF matrix\n",
    "\n",
    "abstracts = []\n",
    "with open(ABSTRACTS_TXT,'r',encoding='utf-8') as f:\n",
    "    for ln in f:\n",
    "        parts = ln.split('|--|')\n",
    "        abstracts.append(parts[1].strip().lower() if len(parts)==2 else ln.strip().lower())\n",
    "\n",
    "tfidf_matrix = tfidf_vect.transform(abstracts)\n",
    "\n",
    "\n",
    "# 3. Load authors\n",
    "\n",
    "paper_auth = {}\n",
    "with open(AUTHORS_PATH,'r',encoding='utf-8') as fh:\n",
    "    for ln in fh:\n",
    "        if '|--|' not in ln: continue\n",
    "        pid, names = ln.rstrip().split('|--|',1)\n",
    "        try:\n",
    "            i = int(pid)\n",
    "            paper_auth[i] = {n.strip() for n in names.split(',') if n.strip()}\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 4. Load test pairs\n",
    "\n",
    "test_pairs = []\n",
    "with open(TEST_TXT,'r') as f:\n",
    "    for ln in f:\n",
    "        ln=ln.strip()\n",
    "        if not ln: continue\n",
    "        u,v = map(int, ln.split(',')[:2])\n",
    "        test_pairs.append((u,v))\n",
    "\n",
    "\n",
    "# 5. Helper fns\n",
    "\n",
    "def tfidf_cosine(u,v):\n",
    "    try:\n",
    "        return float(cosine_similarity(tfidf_matrix[u], tfidf_matrix[v])[0,0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def author_feat_jaccard(u, v):\n",
    "    Au = paper_auth.get(u, set())\n",
    "    Av = paper_auth.get(v, set())\n",
    "    inter = len(Au & Av)\n",
    "    union = len(Au | Av)\n",
    "    return 0.0 if union == 0 else inter/union\n",
    "\n",
    "\n",
    "# 6. Build features\n",
    "\n",
    "X = []\n",
    "for u,v in tqdm(test_pairs, desc='Building features'):\n",
    "    # PCA embeddings\n",
    "   \n",
    "    nu = pca_n2v.transform(n2v_raw[u].reshape(1, -1))[0] if u in n2v_raw else np.zeros(pca_n2v.n_components_)\n",
    "    cu = pca_cls.transform(cls_raw[u].reshape(1, -1))[0] if u < len(cls_raw) else np.zeros(pca_cls.n_components_)\n",
    "    su = pca_bert.transform(bert_raw[u].reshape(1, -1))[0] if u < len(bert_raw) else np.zeros(pca_sci.n_components_)\n",
    "\n",
    "    nv = pca_n2v.transform(n2v_raw[v].reshape(1, -1))[0] if v in n2v_raw else np.zeros(pca_n2v.n_components_)\n",
    "    cv = pca_cls.transform(cls_raw[v].reshape(1, -1))[0] if v < len(cls_raw) else np.zeros(pca_cls.n_components_)\n",
    "    sv = pca_bert.transform(bert_raw[v].reshape(1, -1))[0] if v < len(bert_raw) else np.zeros(pca_sci.n_components_)\n",
    "\n",
    "    eu = np.concatenate([nu, cu, su])\n",
    "    ev = np.concatenate([nv, cv, sv])\n",
    "\n",
    "    # interactions & similarity\n",
    "    diff = np.abs(eu-ev)\n",
    "    prod = eu*ev\n",
    "    sims = float(np.dot(eu,ev)/(np.linalg.norm(eu)*np.linalg.norm(ev)+1e-9))\n",
    "    # graph\n",
    "    g = graph_feats.get((u,v), graph_feats.get((v,u), np.zeros(2,dtype=np.float32)))\n",
    "    # author & tfidf\n",
    "    aj = author_feat_jaccard(u,v)\n",
    "    tc    = tfidf_cosine(u,v)\n",
    "    meta  = np.array([aj,tc,sims],dtype=np.float32)\n",
    "    # concat\n",
    "    X.append(np.concatenate([eu,ev,diff,prod,g,meta]))\n",
    "\n",
    "X = np.stack(X)\n",
    "\n",
    "\n",
    "# 7. Scale, predict & save\n",
    "\n",
    "X_s   = scaler.transform(X)\n",
    "probs = model.predict_proba(X_s)[:,1]\n",
    "\n",
    "import csv\n",
    "with open(OUTPUT_SUB_PATH,'w',newline='') as fout:\n",
    "    w=csv.writer(fout)\n",
    "    w.writerow(['ID','Label'])\n",
    "    for i,p in enumerate(probs):\n",
    "        w.writerow([i,f\"{p:.6f}\"])\n",
    "\n",
    "print(f\"✅ submission saved to {OUTPUT_SUB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-14T13:50:11.782489Z",
     "iopub.status.busy": "2025-06-14T13:50:11.782129Z",
     "iopub.status.idle": "2025-06-14T18:38:46.997669Z",
     "shell.execute_reply": "2025-06-14T18:38:46.993949Z",
     "shell.execute_reply.started": "2025-06-14T13:50:11.782458Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#XGBoost classifier\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, roc_auc_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# 1. Load precomputed raw embeddings and features\n",
    "\n",
    "\n",
    "with open(\"/kaggle/input/scibert/scibert_embeddings.pkl\", \"rb\") as f:\n",
    "    bert_emb_raw = pickle.load(f)    # now SciBERT CLS vectors\n",
    "with open(\"/kaggle/input/n2v-rich/node2vec_embeddings_train (1).pkl\", \"rb\") as f:\n",
    "    n2v_emb_raw = pickle.load(f)\n",
    "with open(\"/kaggle/input/fine-tuned-embed/cls_embeddings (3).npy\", \"rb\") as f:\n",
    "    cls_emb_raw = np.load(f)\n",
    "with open(\"/kaggle/input/graph-feats/graph_features (4).pkl\", \"rb\") as f:\n",
    "    graph_feats = pickle.load(f)\n",
    "\n",
    "\n",
    "# 2. Load abstracts and positive edges\n",
    "-\n",
    "abstracts = []\n",
    "with open(\"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split(\"|--|\")\n",
    "        abstracts.append(parts[1].strip().lower() if len(parts)==2 else line.strip().lower())\n",
    "\n",
    "edges_pos = pd.read_csv(\n",
    "    \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\",\n",
    "    header=None, names=[\"u\", \"v\"], dtype=int\n",
    ")\n",
    "edges_pos[\"y\"] = 1\n",
    "\n",
    "\n",
    "# 3. Load authors mapping\n",
    "\n",
    "def load_authors(fp: str) -> dict:\n",
    "    d = {}\n",
    "    with open(fp, encoding=\"utf-8\") as fh:\n",
    "        for ln in fh:\n",
    "            if \"|--|\" not in ln:\n",
    "                continue\n",
    "            pid_str, names = ln.rstrip().split(\"|--|\",1)\n",
    "            try:\n",
    "                pid = int(pid_str)\n",
    "                d[pid] = {n.strip() for n in names.split(\",\") if n.strip()}\n",
    "            except:\n",
    "                pass\n",
    "    return d\n",
    "\n",
    "paper_auth = load_authors(\"/kaggle/input/nlp-cse-uoi-2025/data_new/authors.txt\")\n",
    "\n",
    "\n",
    "# 4. Sample negatives and train/test split\n",
    "\n",
    "def sample_negatives(df, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_set = set(map(tuple, df[[\"u\",\"v\"]].values))\n",
    "    N = df[[\"u\",\"v\"]].values.max() + 1\n",
    "    neg = set()\n",
    "    while len(neg) < len(df):\n",
    "        u, v = rng.integers(0, N, size=2)\n",
    "        if u != v and (u, v) not in pos_set and (v, u) not in pos_set:\n",
    "            neg.add((u, v))\n",
    "    return pd.DataFrame(list(neg), columns=[\"u\",\"v\"]), N\n",
    "\n",
    "edges_neg, num_nodes = sample_negatives(edges_pos)\n",
    "edges_neg[\"y\"] = 0\n",
    "\n",
    "full = pd.concat([edges_pos, edges_neg], ignore_index=True)\n",
    "train_df, test_df = train_test_split(full, test_size=0.2, stratify=full.y, random_state=42)\n",
    "\n",
    "# 5. Fit PCA on train node embeddings\n",
    "\n",
    "train_nodes = pd.unique(train_df[[\"u\",\"v\"]].values.ravel())\n",
    "bert_mat = np.stack([bert_emb_raw[n] for n in train_nodes], axis=0)\n",
    "n2v_mat  = np.stack([n2v_emb_raw[n] for n in train_nodes], axis=0)\n",
    "cls_mat  = np.stack([cls_emb_raw[n] for n in train_nodes], axis=0)\n",
    "\n",
    "pca_bert = PCA(n_components=56, random_state=42).fit(bert_mat)   # SciBERT PCA\n",
    "pca_n2v  = PCA(n_components=100, random_state=42).fit(n2v_mat)\n",
    "pca_cls  = PCA(n_components=40, random_state=42).fit(cls_mat)\n",
    "\n",
    "bert_emb = {n: pca_bert.transform(bert_emb_raw[n].reshape(1,-1))[0] for n in bert_emb_raw}\n",
    "n2v_emb  = {n: pca_n2v.transform(n2v_emb_raw[n].reshape(1,-1))[0]  for n in n2v_emb_raw}\n",
    "cls_emb  = {n: pca_cls.transform(cls_emb_raw[n].reshape(1,-1))[0]      for n in range(len(cls_emb_raw))}\n",
    "\n",
    "\n",
    "# 6. Fit TF-IDF on abstracts\n",
    "\n",
    "vectorizer   = TfidfVectorizer(max_features=10000, stop_words='english', min_df=20)\n",
    "train_texts  = [abstracts[n] for n in train_nodes]\n",
    "vectorizer.fit(train_texts)\n",
    "tfidf_matrix = vectorizer.transform(abstracts)\n",
    "\n",
    "\n",
    "# 7. Feature builder\n",
    "\n",
    "def tfidf_cosine(u, v):\n",
    "    try:\n",
    "        return float(cosine_similarity(tfidf_matrix[u], tfidf_matrix[v])[0,0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def author_feat_jaccard(u, v):\n",
    "    Au = paper_auth.get(u, set())\n",
    "    Av = paper_auth.get(v, set())\n",
    "    inter = len(Au & Av)\n",
    "    union = len(Au | Av)\n",
    "    return 0.0 if union == 0 else inter / union\n",
    "\n",
    "def build_features(df):\n",
    "    X = []\n",
    "    for u, v in tqdm(df[['u','v']].itertuples(index=False), total=len(df), desc=\"Building features\"):\n",
    "        # combine node2vec + CLS + SciBERT PCA embeddings\n",
    "        eu = np.concatenate([n2v_emb[u], cls_emb[u], bert_emb[u]])\n",
    "        ev = np.concatenate([n2v_emb[v], cls_emb[v], bert_emb[v]])\n",
    "\n",
    "        # diff & product\n",
    "        diff = np.abs(eu - ev)\n",
    "        prod = eu * ev\n",
    "        \n",
    "        # cosine similarity\n",
    "        sim = float(np.dot(eu, ev) /\n",
    "                    (np.linalg.norm(eu) * np.linalg.norm(ev) + 1e-9))\n",
    "        \n",
    "        # graph-based features\n",
    "        g = graph_feats.get((u, v),\n",
    "                            graph_feats.get((v, u),\n",
    "                                            np.zeros(2, dtype=np.float32)))\n",
    "        \n",
    "        # meta-features\n",
    "        aj = author_feat_jaccard(u, v)\n",
    "        tc = tfidf_cosine(u, v)\n",
    "        meta = np.array([aj, tc, sim], dtype=np.float32)\n",
    "        \n",
    "        X.append(np.concatenate([eu, ev, diff, prod, g, meta]))\n",
    "    return np.stack(X)\n",
    "\n",
    "X_train = build_features(train_df)\n",
    "X_test  = build_features(test_df)\n",
    "y_train = train_df.y.values\n",
    "y_test  = test_df.y.values\n",
    "\n",
    "\n",
    "# 8. Train & eval with XGBoost\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,  gg\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_test_scaled, y_test)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "probs = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "preds = (probs > 0.5).astype(int)\n",
    "\n",
    "print('Log-loss:', log_loss(y_test, probs))\n",
    "print('AUROC   :', roc_auc_score(y_test, probs))\n",
    "print('F1      :', f1_score(y_test, preds))\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Save artifacts\n",
    "# ----------------------------\n",
    "with open('pca_bert.pkl','wb')   as f: pickle.dump(pca_bert, f)\n",
    "with open('pca_n2v.pkl','wb')    as f: pickle.dump(pca_n2v, f)\n",
    "with open('pca_cls.pkl','wb')    as f: pickle.dump(pca_cls, f)\n",
    "with open('tfidf_vect.pkl','wb') as f: pickle.dump(vectorizer, f)\n",
    "with open('scaler.pkl','wb')     as f: pickle.dump(scaler, f)\n",
    "with open('xgb_model.pkl','wb')  as f: pickle.dump(clf, f)\n",
    "\n",
    "print('✅ Saved all artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T19:11:16.508616Z",
     "iopub.status.busy": "2025-06-14T19:11:16.508150Z",
     "iopub.status.idle": "2025-06-14T19:14:12.530197Z",
     "shell.execute_reply": "2025-06-14T19:14:12.528668Z",
     "shell.execute_reply.started": "2025-06-14T19:11:16.508586Z"
    }
   },
   "outputs": [],
   "source": [
    "#TESTING SCRIPT FOR MY XGBoost\n",
    "\n",
    "\n",
    "# Config & paths\n",
    "\n",
    "TEST_TXT         = '/kaggle/input/nlp-cse-uoi-2025/data_new/test.txt'\n",
    "ABSTRACTS_TXT    = '/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt'\n",
    "AUTHORS_PATH     = '/kaggle/input/nlp-cse-uoi-2025/data_new/authors.txt'\n",
    "\n",
    "PCA_N2V_PATH     = 'pca_n2v.pkl'\n",
    "PCA_CLS_PATH     = 'pca_cls.pkl'\n",
    "PCA_BERT_PATH    = 'pca_bert.pkl'        \n",
    "TFIDF_VECT_PATH  = 'tfidf_vect.pkl'\n",
    "SCALER_PATH      = 'scaler.pkl'\n",
    "MODEL_PATH       = 'xgb_model.pkl'\n",
    "GRAPH_FEAT_PATH  = '/kaggle/working/graph_features_test.pkl'\n",
    "\n",
    "OUTPUT_SUB_PATH  = 'submission.csv'\n",
    "\n",
    "# 1. Load preprocessing artifacts & model\n",
    "\n",
    "with open(PCA_N2V_PATH,    'rb') as f: pca_n2v    = pickle.load(f)\n",
    "with open(PCA_CLS_PATH,    'rb') as f: pca_cls    = pickle.load(f)\n",
    "with open(PCA_BERT_PATH,   'rb') as f: pca_bert   = pickle.load(f)    \n",
    "with open(TFIDF_VECT_PATH, 'rb') as f: tfidf_vect = pickle.load(f)\n",
    "with open(SCALER_PATH,     'rb') as f: scaler     = pickle.load(f)\n",
    "with open(MODEL_PATH,      'rb') as f: model      = pickle.load(f)\n",
    "with open(GRAPH_FEAT_PATH, 'rb') as f: graph_feats= pickle.load(f)\n",
    "\n",
    "\n",
    "# 2. Load raw embeddings\n",
    "\n",
    "with open('/kaggle/input/scibert/scibert_embeddings.pkl','rb') as f:\n",
    "    bert_raw = pickle.load(f)                     # ← new\n",
    "with open('/kaggle/input/n2v-rich/node2vec_embeddings_train (1).pkl','rb') as f:\n",
    "    n2v_raw  = pickle.load(f)\n",
    "cls_raw = np.load('/kaggle/input/fine-tuned-embed/cls_embeddings (3).npy')\n",
    "\n",
    "\n",
    "# 3. Load abstracts & build TF-IDF matrix\n",
    "\n",
    "abstracts = []\n",
    "with open(ABSTRACTS_TXT, 'r', encoding='utf-8') as f:\n",
    "    for ln in f:\n",
    "        parts = ln.split('|--|')\n",
    "        abstracts.append(parts[1].strip().lower() if len(parts)==2 else ln.strip().lower())\n",
    "\n",
    "tfidf_matrix = tfidf_vect.transform(abstracts)\n",
    "\n",
    "\n",
    "# 4. Load authors mapping\n",
    "\n",
    "paper_auth = {}\n",
    "with open(AUTHORS_PATH, 'r', encoding='utf-8') as fh:\n",
    "    for ln in fh:\n",
    "        if '|--|' not in ln:\n",
    "            continue\n",
    "        pid, names = ln.rstrip().split('|--|', 1)\n",
    "        try:\n",
    "            i = int(pid)\n",
    "            paper_auth[i] = {n.strip() for n in names.split(',') if n.strip()}\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 5. Load test pairs\n",
    "\n",
    "test_pairs = []\n",
    "with open(TEST_TXT, 'r') as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        u, v = map(int, ln.split(',')[:2])\n",
    "        test_pairs.append((u, v))\n",
    "\n",
    "\n",
    "# 6. Helper fns\n",
    "\n",
    "def tfidf_cosine(u, v):\n",
    "    try:\n",
    "        return float(cosine_similarity(tfidf_matrix[u], tfidf_matrix[v])[0,0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def author_feat_jaccard(u, v):\n",
    "    Au = paper_auth.get(u, set())\n",
    "    Av = paper_auth.get(v, set())\n",
    "    inter = len(Au & Av)\n",
    "    union = len(Au | Av)\n",
    "    return 0.0 if union == 0 else inter / union\n",
    "\n",
    "\n",
    "# 7. Build features for test set\n",
    "\n",
    "X = []\n",
    "for u, v in tqdm(test_pairs, desc='Building features'):\n",
    "    # PCA embeddings for node2vec, CLS &  Sci-BERT\n",
    "    nu = pca_n2v.transform(n2v_raw[u].reshape(1, -1))[0]  if u in n2v_raw else np.zeros(pca_n2v.n_components_)\n",
    "    nv = pca_n2v.transform(n2v_raw[v].reshape(1, -1))[0]  if v in n2v_raw else np.zeros(pca_n2v.n_components_)\n",
    "    cu = pca_cls.transform(cls_raw[u].reshape(1, -1))[0]  if u < len(cls_raw) else np.zeros(pca_cls.n_components_)\n",
    "    cv = pca_cls.transform(cls_raw[v].reshape(1, -1))[0]  if v < len(cls_raw) else np.zeros(pca_cls.n_components_)\n",
    "    bu = pca_bert.transform(bert_raw[u].reshape(1, -1))[0] if u in bert_raw else np.zeros(pca_bert.n_components_)\n",
    "    bv = pca_bert.transform(bert_raw[v].reshape(1, -1))[0] if v in bert_raw else np.zeros(pca_bert.n_components_)\n",
    "\n",
    "    # final “embedding” vectors\n",
    "    eu = np.concatenate([nu, cu,bu])\n",
    "    ev = np.concatenate([nv, cv,bv])\n",
    "\n",
    " \n",
    "    diff = np.abs(eu - ev)\n",
    "    prod = eu * ev\n",
    "  \n",
    "    sim = float(np.dot(eu, ev) /\n",
    "                    (np.linalg.norm(eu) * np.linalg.norm(ev) + 1e-9))\n",
    "        \n",
    "     \n",
    "    g = graph_feats.get((u, v), graph_feats.get((v, u), np.zeros(2, dtype=np.float32)))\n",
    "    aj = author_feat_jaccard(u, v)\n",
    "    tc = tfidf_cosine(u, v)\n",
    "    meta = np.array([aj, tc, sim], dtype=np.float32)\n",
    "\n",
    "        # Now all components are 1D arrays\n",
    "    X.append(np.concatenate([eu, ev, diff, prod, g, meta]))\n",
    "X = np.stack(X)\n",
    "# ----------------------------\n",
    "# 8. Scale, predict & save\n",
    "# ----------------------------\n",
    "X_s   = scaler.transform(X)\n",
    "probs = model.predict_proba(X_s)[:, 1]\n",
    "\n",
    "import csv\n",
    "with open(OUTPUT_SUB_PATH, 'w', newline='') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['ID', 'Label'])\n",
    "    for i, p in enumerate(probs):\n",
    "        writer.writerow([i, f\"{p:.6f}\"])\n",
    "\n",
    "print(f\" submission saved to {OUTPUT_SUB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-14T19:47:40.524Z",
     "iopub.execute_input": "2025-06-14T19:44:48.082363Z",
     "iopub.status.busy": "2025-06-14T19:44:48.080827Z"
    }
   },
   "outputs": [],
   "source": [
    "#CNN MODEL USING PYTORCH\n",
    "# ------------------------------------------------------------------\n",
    "# 0) Imports\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# 1) Load data\n",
    "\n",
    "AUTHORS = \"/kaggle/input/nlp-cse-uoi-2025/data_new/authors.txt\"\n",
    "EDGES   = \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\"\n",
    "BERT_EMB = \"/kaggle/input/scibert/scibert_embeddings.pkl\"\n",
    "NODE2VEC_EMB = \"/kaggle/input/n2v-rich/node2vec_embeddings_train (1).pkl\"\n",
    "GRAPH_FEAT = \"/kaggle/input/gfeat-with-3-feats/graph_features_enriched_leakfree (4).pkl\"\n",
    "CLS_EMB_NPY = \"/kaggle/input/fine-tuned-embed/cls_embeddings (3).npy\"\n",
    "ABSTRACTS_TXT = \"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\"\n",
    "\n",
    "with open(BERT_EMB, \"rb\") as f:\n",
    "    bert_embeddings = pickle.load(f)\n",
    "print(f\"Loaded {len(bert_embeddings)} SBERT embeddings.\")\n",
    "\n",
    "with open(NODE2VEC_EMB, \"rb\") as f:\n",
    "    node2vec_embeddings = pickle.load(f)\n",
    "print(f\"Loaded {len(node2vec_embeddings)} Node2Vec embeddings.\")\n",
    "\n",
    "with open(GRAPH_FEAT, \"rb\") as f:\n",
    "    graph_features = pickle.load(f)\n",
    "print(f\"Loaded {len(graph_features)} graph features.\")\n",
    "\n",
    "cls_np = np.load(CLS_EMB_NPY)\n",
    "cls_embeddings = {pid: cls_np[i] for i, pid in enumerate(bert_embeddings.keys())}\n",
    "\n",
    "# Load abstracts for TF-IDF\n",
    "abstracts = []\n",
    "with open(ABSTRACTS_TXT, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split(\"|--|\")\n",
    "        text = parts[1].strip().lower() if len(parts) == 2 else line.strip().lower()\n",
    "        abstracts.append(text)\n",
    "\n",
    "\n",
    "# 2) Load authors and edges\n",
    "\n",
    "def load_authors(fp):\n",
    "    d = {}\n",
    "    with open(fp, encoding=\"utf-8\") as fh:\n",
    "        for ln in fh:\n",
    "            if \"|--|\" not in ln: continue\n",
    "            pid, names = ln.rstrip().split(\"|--|\", 1)\n",
    "            try: pid = int(pid)\n",
    "            except ValueError: continue\n",
    "            d[pid] = {n.strip() for n in names.split(\",\") if n.strip()}\n",
    "    return d\n",
    "\n",
    "paper_auth = load_authors(AUTHORS)\n",
    "edges_pos  = pd.read_csv(EDGES, header=None, names=[\"u\", \"v\"])\n",
    "N          = len(bert_embeddings)\n",
    "\n",
    "print(f\"Total papers with SBERT embeddings: {N} | Positives: {len(edges_pos):,}\")\n",
    "\n",
    "\n",
    "# 3) Negative sampling\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "neg = set()\n",
    "\n",
    "\n",
    "edges_pos_set = set(tuple(sorted(edge)) for edge in edges_pos)\n",
    "\n",
    "while len(neg) < len(edges_pos):\n",
    "    a, b = rng.integers(N, size=2)\n",
    "    if a != b:\n",
    "        edge = tuple(sorted((a, b)))  # unordered pair\n",
    "        if edge not in edges_pos_set:\n",
    "            neg.add(edge)\n",
    "\n",
    "edges_neg = pd.DataFrame(list(neg), columns=[\"u\", \"v\"])\n",
    "\n",
    "\n",
    "\n",
    "# 4) Author-overlap helper\n",
    "\n",
    "def author_feats(u, v):\n",
    "    Au, Av = paper_auth.get(u, ()), paper_auth.get(v, ())\n",
    "    inter = len(Au & Av)\n",
    "    if not (Au or Av): return 0, 0.0\n",
    "    jac = inter / (len(Au) + len(Av) - inter) if inter else 0.0\n",
    "    return inter, jac\n",
    "\n",
    "\n",
    "# 5) TF-IDF similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "def tfidf_cosine(u, v):\n",
    "    try:\n",
    "        return float(cosine_similarity(tfidf_matrix[u], tfidf_matrix[v])[0, 0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# 6) Dataset with SBERT + Node2Vec + Graph Feats + CLS Cosine\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, df, y):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.y  = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u, v = self.df.iloc[idx]\n",
    "\n",
    "        vu_sbert = bert_embeddings.get(int(u), np.zeros(768, dtype=np.float32))\n",
    "        vv_sbert = bert_embeddings.get(int(v), np.zeros(768, dtype=np.float32))\n",
    "        vu_n2v = node2vec_embeddings.get(int(u), np.zeros(128, dtype=np.float32))\n",
    "        vv_n2v = node2vec_embeddings.get(int(v), np.zeros(128, dtype=np.float32))\n",
    "        vu_cls = cls_embeddings.get(int(u), np.zeros(768, dtype=np.float32))\n",
    "        vv_cls = cls_embeddings.get(int(v), np.zeros(768, dtype=np.float32))\n",
    "\n",
    "        vu = np.concatenate([vu_sbert, vu_n2v])\n",
    "        vv = np.concatenate([vv_sbert, vv_n2v])\n",
    "\n",
    "        x6 = np.stack([vu, vv, np.abs(vu - vv), vu * vv], axis=0).astype(np.float32)\n",
    "\n",
    "        inter, jac = author_feats(int(u), int(v))\n",
    "        cos = float(np.dot(vu, vv) / (np.linalg.norm(vu) * np.linalg.norm(vv) + 1e-9))\n",
    "        eucl = float(np.linalg.norm(vu - vv))\n",
    "        sim_bert = cos\n",
    "        tfidf_sim = tfidf_cosine(int(u), int(v))\n",
    "        cos_cls = float(np.dot(vu_cls, vv_cls) / (np.linalg.norm(vu_cls) * np.linalg.norm(vv_cls) + 1e-9))\n",
    "        gfeat = graph_features.get((int(u), int(v)), np.zeros(3, dtype=np.float32))\n",
    "\n",
    "        meta = np.concatenate([np.array([inter, jac, cos, eucl, sim_bert, tfidf_sim, cos_cls], dtype=np.float32), gfeat])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x6),\n",
    "            torch.from_numpy(meta),\n",
    "            torch.tensor(self.y[idx])\n",
    "        )\n",
    "\n",
    "\n",
    "# 7) Train/test split & loaders\n",
    "\n",
    "full_X = pd.concat([edges_pos, edges_neg], ignore_index=True)\n",
    "full_y = np.concatenate([np.ones(len(edges_pos)), np.zeros(len(edges_neg))])\n",
    "\n",
    "tr_idx, te_idx = train_test_split(np.arange(len(full_y)),\n",
    "                                  test_size=0.2,\n",
    "                                  stratify=full_y,\n",
    "                                  random_state=42)\n",
    "\n",
    "train_ds = PairDataset(full_X.iloc[tr_idx], full_y[tr_idx])\n",
    "test_ds  = PairDataset(full_X.iloc[te_idx],  full_y[te_idx])\n",
    "\n",
    "BATCH = 128\n",
    "train_loader = DataLoader(train_ds, BATCH, shuffle=True,  num_workers=4, drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  BATCH, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# 8) CNN (input = 4 channels, 448 dims per emb)\n",
    "\n",
    "class PairCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(4, 32, 5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        # Automatically calculate output dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 4, 896)  # 896 = input dim per emb\n",
    "            out_dim = self.conv(dummy).flatten(1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out_dim + 10, 128),  # 10 meta features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x6, meta):\n",
    "        z = self.conv(x6).flatten(1)\n",
    "        z = torch.cat([z, meta], dim=1)\n",
    "        return self.fc(z).squeeze(1)\n",
    "\n",
    "model = PairCNN().to(device)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "lossf = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# 9) Train with best model tracking\n",
    "\n",
    "EPOCHS =12\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train(); tr_losses = []\n",
    "    for x6, meta, y in tqdm(train_loader, desc=f\"ep{ep}\", leave=False):\n",
    "        x6, meta, y = x6.to(device), meta.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(x6, meta)\n",
    "        loss = lossf(logits, y)\n",
    "        loss.backward(); opt.step()\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    model.eval(); probs, y_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for x6, meta, y in test_loader:\n",
    "            logits = model(x6.to(device), meta.to(device))\n",
    "            probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            y_true.append(y.numpy())\n",
    "\n",
    "    probs = np.concatenate(probs)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = (probs > 0.5).astype(np.uint8)\n",
    "\n",
    "    val_auc = roc_auc_score(y_true, probs)\n",
    "    val_log = log_loss(y_true, probs)\n",
    "    val_f1  = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Epoch {ep} | Train loss: {np.mean(tr_losses):.4f} | \"\n",
    "          f\"Val AUROC: {val_auc:.4f} | F1: {val_f1:.4f} | Log-loss: {val_log:.4f}\")\n",
    "\n",
    "    if val_log < best_loss:\n",
    "        best_loss = val_log\n",
    "        torch.save(model.state_dict(), \"best_model_sbert_n2v.pt\")\n",
    "        print(f\" Saved model at epoch {ep} with log loss {val_log:.4f}\")\n",
    "\n",
    "\n",
    "# 10) Final Evaluation\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model_sbert_n2v.pt\"))\n",
    "model.eval(); probs = []\n",
    "with torch.no_grad():\n",
    "    for x6, meta, _ in test_loader:\n",
    "        logits = model(x6.to(device), meta.to(device))\n",
    "        probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "probs = np.concatenate(probs)\n",
    "\n",
    "print(\"Final evaluation on test set:\")\n",
    "print(\"AUROC   :\", roc_auc_score(full_y[te_idx], probs))\n",
    "print(\"Log-loss:\", log_loss(full_y[te_idx],  probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN TESTING SCRIPT\n",
    "\n",
    "# 1) Load all resources\n",
    "\n",
    "\n",
    "# Paths\n",
    "BERT_EMB     = \"/kaggle/input/bert-embed/bert_embeddings.pkl\"\n",
    "NODE2VEC_EMB = \"/kaggle/working/node2vec_embeddings_train80.pkl\"\n",
    "CLS_EMB_NPY  = \"/kaggle/input/embeddings/cls_embeddings.npy\"\n",
    "GRAPH_FEAT   = \"/kaggle/working/graph_features_test.pkl\"\n",
    "AUTHORS_PATH = \"/kaggle/input/nlp-cse-uoi-2025/data_new/authors.txt\"\n",
    "TEST_TXT     = \"/kaggle/input/nlp-cse-uoi-2025/data_new/test.txt\"\n",
    "MODEL_PATH   = \"/kaggle/working/best_model_sbert_n2v.pt\"\n",
    "ABSTRACTS_TXT = \"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\"\n",
    "\n",
    "# Load embeddings\n",
    "with open(BERT_EMB, \"rb\") as f:\n",
    "    bert_embeddings = pickle.load(f)\n",
    "with open(NODE2VEC_EMB, \"rb\") as f:\n",
    "    node2vec_embeddings = pickle.load(f)\n",
    "cls_np = np.load(CLS_EMB_NPY)\n",
    "cls_embeddings = {pid: cls_np[i] for i, pid in enumerate(bert_embeddings.keys())}\n",
    "with open(GRAPH_FEAT, \"rb\") as f:\n",
    "    graph_features = pickle.load(f)\n",
    "\n",
    "# Load authors\n",
    "paper_auth = {}\n",
    "with open(AUTHORS_PATH, encoding=\"utf-8\") as fh:\n",
    "    for ln in fh:\n",
    "        if \"|--|\" in ln:\n",
    "            pid, names = ln.rstrip().split(\"|--|\", 1)\n",
    "            try:\n",
    "                paper_auth[int(pid)] = {n.strip() for n in names.split(\",\") if n.strip()}\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "# Load abstracts and compute TF-IDF\n",
    "abstracts = []\n",
    "with open(ABSTRACTS_TXT, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split(\"|--|\")\n",
    "        text = parts[1].strip().lower() if len(parts) == 2 else line.strip().lower()\n",
    "        abstracts.append(text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "\n",
    "# 2) CNN Model — Matches Saved Model Architecture\n",
    "\n",
    "class PairCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(4, 32, 5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 112 + 9, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x6, meta):\n",
    "        z = self.conv(x6).flatten(1)\n",
    "        z = torch.cat([z, meta], dim=1)\n",
    "        return self.fc(z).squeeze(1)\n",
    "\n",
    "\n",
    "# 3) Load pretrained model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PairCNN().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "print(\"✅ Model loaded.\")\n",
    "\n",
    "\n",
    "# 4) Utility functions\n",
    "\n",
    "def load_test_pairs(fp):\n",
    "    pairs = []\n",
    "    with open(fp, encoding=\"utf-8\") as fh:\n",
    "        for ln in fh:\n",
    "            if ln.strip():\n",
    "                try:\n",
    "                    u, v = map(int, ln.strip().split(\",\")[:2])\n",
    "                    pairs.append((u, v))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return pairs\n",
    "\n",
    "def author_feats(u, v):\n",
    "    Au, Av = paper_auth.get(u, set()), paper_auth.get(v, set())\n",
    "    inter = len(Au & Av)\n",
    "    if not (Au or Av): return 0, 0.0\n",
    "    jac = inter / (len(Au) + len(Av) - inter) if inter else 0.0\n",
    "    return inter, jac\n",
    "\n",
    "def tfidf_cosine(u, v):\n",
    "    try:\n",
    "        return float(cosine_similarity(tfidf_matrix[u], tfidf_matrix[v])[0, 0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# 5) Dataset for inference — Matches saved model input\n",
    "\n",
    "class TestPairDS(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u, v = self.pairs[idx]\n",
    "\n",
    "        vu_sbert = bert_embeddings.get(u, np.zeros(384, dtype=np.float32))\n",
    "        vv_sbert = bert_embeddings.get(v, np.zeros(384, dtype=np.float32))\n",
    "        vu_n2v   = node2vec_embeddings.get(u, np.zeros(64, dtype=np.float32))\n",
    "        vv_n2v   = node2vec_embeddings.get(v, np.zeros(64, dtype=np.float32))\n",
    "        vu_cls   = cls_embeddings.get(u, np.zeros(768, dtype=np.float32))\n",
    "        vv_cls   = cls_embeddings.get(v, np.zeros(768, dtype=np.float32))\n",
    "\n",
    "        vu = np.concatenate([vu_sbert, vu_n2v])\n",
    "        vv = np.concatenate([vv_sbert, vv_n2v])\n",
    "        x6 = np.stack([vu, vv, np.abs(vu - vv), vu * vv], axis=0).astype(np.float32)\n",
    "\n",
    "        inter, jac = author_feats(u, v)\n",
    "        cos        = float(np.dot(vu, vv) / (np.linalg.norm(vu) * np.linalg.norm(vv) + 1e-9))\n",
    "        eucl       = float(np.linalg.norm(vu - vv))\n",
    "        tfidf_sim  = tfidf_cosine(u, v)\n",
    "        cos_cls    = float(np.dot(vu_cls, vv_cls) / (np.linalg.norm(vu_cls) * np.linalg.norm(vv_cls) + 1e-9))\n",
    "\n",
    "        gfeat = graph_features.get((u, v))\n",
    "        if gfeat is None:\n",
    "            gfeat = graph_features.get((v, u))\n",
    "        if gfeat is None:\n",
    "            gfeat = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "        meta = np.concatenate([np.array([inter, jac, cos, eucl, cos, tfidf_sim, cos_cls], dtype=np.float32), gfeat])\n",
    "        return torch.from_numpy(x6), torch.from_numpy(meta)\n",
    "\n",
    "\n",
    "# 6) Inference\n",
    "\n",
    "test_pairs = load_test_pairs(TEST_TXT)\n",
    "loader = DataLoader(TestPairDS(test_pairs), batch_size=1024, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"🔍 Loaded {len(test_pairs):,} test pairs.\")\n",
    "\n",
    "probs = []\n",
    "with torch.no_grad():\n",
    "    for x6, meta in tqdm(loader, desc=\"Inference\"):\n",
    "        x6, meta = x6.to(device), meta.to(device)\n",
    "        logits = model(x6, meta)\n",
    "        probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "\n",
    "y_prob = np.concatenate(probs)\n",
    "print(\" Predictions shape:\", y_prob.shape)\n",
    "print(\" Min:\", y_prob.min(), \"Max:\", y_prob.max(), \"Mean:\", y_prob.mean())\n",
    "\n",
    "\n",
    "# 7) Save predictions\n",
    "\n",
    "with open(\"test_predictions.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"ID\", \"Label\"])\n",
    "    for i, p in enumerate(y_prob):\n",
    "        w.writerow([i, f\"{p:.6f}\"])\n",
    "\n",
    "print(\" Saved predictions to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T22:41:12.804693Z",
     "iopub.status.busy": "2025-06-14T22:41:12.804317Z",
     "iopub.status.idle": "2025-06-14T22:41:30.959511Z",
     "shell.execute_reply": "2025-06-14T22:41:30.958579Z",
     "shell.execute_reply.started": "2025-06-14T22:41:12.804668Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Required for 3D plots\n",
    "\n",
    "# Load embeddings\n",
    "embedding_path = \"/kaggle/input/n2v-rich/node2vec_embeddings_train (1).pkl\"  # Replace if needed\n",
    "with open(embedding_path, \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "# Convert to matrix\n",
    "node_ids = list(embeddings.keys())\n",
    "X = np.array([embeddings[nid] for nid in node_ids])\n",
    "\n",
    "# PCA to 3D\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Clustering for color\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(\n",
    "    X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "    c=labels, cmap='viridis', s=10, alpha=0.6  # Transparency set here\n",
    ")\n",
    "\n",
    "ax.set_title(\"3D PCA Visualization of Node2Vec Embeddings\")\n",
    "ax.set_xlabel(\"Principal Component 1\")\n",
    "ax.set_ylabel(\"Principal Component 2\")\n",
    "ax.set_zlabel(\"Principal Component 3\")\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Save and display\n",
    "output_path = \"node2vec_pca_3d_clusters.png\"\n",
    "plt.savefig(output_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"3D PCA visualization saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11214388,
     "sourceId": 93866,
     "sourceType": "competition"
    },
    {
     "datasetId": 7471155,
     "sourceId": 11886828,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7471215,
     "sourceId": 11886909,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7471257,
     "sourceId": 11886973,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7471586,
     "sourceId": 11887457,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7511761,
     "sourceId": 11948496,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7512721,
     "sourceId": 11949868,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7513737,
     "sourceId": 11951366,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7520156,
     "sourceId": 11959910,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7540736,
     "sourceId": 11988974,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7541110,
     "sourceId": 11989507,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7545431,
     "sourceId": 11995595,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7661658,
     "sourceId": 12164874,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
