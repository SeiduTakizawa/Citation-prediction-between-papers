{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:39:03.876691Z",
     "iopub.status.busy": "2025-05-25T11:39:03.875905Z",
     "iopub.status.idle": "2025-05-25T11:41:17.202564Z",
     "shell.execute_reply": "2025-05-25T11:41:17.201658Z",
     "shell.execute_reply.started": "2025-05-25T11:39:03.876660Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths \n",
    "abstracts_path = \"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\"\n",
    "edgelist_path =  \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\"\n",
    "\n",
    "# 1. Load abstracts\n",
    "abstracts = {}\n",
    "with open(abstracts_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pid, txt = line.strip().split(\"|--|\", maxsplit=1)\n",
    "        abstracts[int(pid)] = txt.strip()\n",
    "\n",
    "# 2. Build positive pairs\n",
    "pos = []\n",
    "pos_set = set()\n",
    "with open(edgelist_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        a, b = map(int, line.strip().split(\",\"))\n",
    "        pos.append({\n",
    "            \"sentence1\": abstracts[a],\n",
    "            \"sentence2\": abstracts[b],\n",
    "            \"label\": 1\n",
    "        })\n",
    "        pos_set.add((a, b))\n",
    "\n",
    "df_pos = pd.DataFrame(pos)\n",
    "\n",
    "# 3. Generate negative pairs (same count as positives)\n",
    "all_ids = list(abstracts.keys())\n",
    "neg = set()\n",
    "while len(neg) < len(df_pos):\n",
    "    a, b = random.sample(all_ids, 2)\n",
    "    if (a, b) not in pos_set and (b,a) not in pos_set:\n",
    "        neg.add((a, b))\n",
    "\n",
    "df_neg = pd.DataFrame([\n",
    "    {\"sentence1\": abstracts[a], \"sentence2\": abstracts[b], \"label\": 0}\n",
    "    for a, b in neg\n",
    "])\n",
    "\n",
    "# 4. Combine & shuffle\n",
    "df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 5. Train/dev split (85/15, stratified by label)\n",
    "train_df, dev_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# 6. Save CSVs\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "dev_df.to_csv( \"dev.csv\",   index=False)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Dev size: {len(dev_df)}\")\n",
    "print(\"\\nSample from train.csv:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nSample from dev.csv:\")\n",
    "print(dev_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:41:17.203918Z",
     "iopub.status.busy": "2025-05-25T11:41:17.203701Z",
     "iopub.status.idle": "2025-05-25T15:59:42.657861Z",
     "shell.execute_reply": "2025-05-25T15:59:42.656607Z",
     "shell.execute_reply.started": "2025-05-25T11:41:17.203901Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = \"distilbert-base-uncased\"\n",
    "TRAIN_FILE,  VALID_FILE  = \"/kaggle/working/train.csv\", \"/kaggle/working/dev.csv\"\n",
    "OUTPUT_DIR    = \"./distilbert-finetuned\"\n",
    "NUM_EPOCHS    = 3\n",
    "BATCH_SIZE    = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH    = 128\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"     # local cache\n",
    "os.environ[\"WANDB_DISABLED\"]    = \"true\"           # no wandb prompt\n",
    "\n",
    "\n",
    "# 1. Load CSVs\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": TRAIN_FILE, \"validation\": VALID_FILE},\n",
    "    cache_dir=\"./hf_cache\",\n",
    ")\n",
    "\n",
    "# 2. Tokeniser + preprocessing\n",
    "tok = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def preprocess(batch):\n",
    "    enc = tok(\n",
    "        [str(x) for x in batch[\"sentence1\"]],\n",
    "        [str(x) for x in batch[\"sentence2\"]],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    enc[\"labels\"] = [float(y) for y in batch[\"label\"]]   # regression target\n",
    "    return enc\n",
    "\n",
    "ds_tok = ds.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence1\", \"sentence2\", \"label\"],  # drop raw text/old label\n",
    ")\n",
    "\n",
    "collate = DataCollatorWithPadding(tok)\n",
    "\n",
    "# 3. Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    problem_type=\"regression\",\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "# 4. Metric (NumPy Pearson)\n",
    "def compute_metrics(pred):\n",
    "    p, l = pred\n",
    "    return {\"pearson\": float(np.corrcoef(p.flatten(), l.astype(float))[0, 1])}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    do_train=True, do_eval=True,\n",
    "    num_train_epochs=1,                \n",
    "    per_device_train_batch_size=32,    \n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"pearson\",\n",
    "    save_total_limit=1,\n",
    "    greater_is_better=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=True,                         \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 6. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collate,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# 7. Train & save\n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "# ——— LOCAL OUTPUT_DIR ———\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T16:09:59.828251Z",
     "iopub.status.busy": "2025-05-25T16:09:59.827881Z",
     "iopub.status.idle": "2025-05-25T16:10:01.773141Z",
     "shell.execute_reply": "2025-05-25T16:10:01.772305Z",
     "shell.execute_reply.started": "2025-05-25T16:09:59.828216Z"
    }
   },
   "outputs": [],
   "source": [
    "#abstract text cleaning for upper case and delimeters\n",
    "cleaned_abstracts = []\n",
    "with open(\"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "       \n",
    "        parts = line.split(\"|--|\")\n",
    "        if len(parts) == 2:\n",
    "            abstract_text = parts[1].strip()\n",
    "        else:\n",
    "            abstract_text = line.strip() \n",
    "        # 2) Convert to lowercase if using an uncased model\n",
    "        abstract_text = abstract_text.lower()\n",
    "\n",
    "        cleaned_abstracts.append(abstract_text)\n",
    "    print(f\"Abstract {1}:\\n{abstract_text}\\n\")\n",
    "    size=len(cleaned_abstracts)\n",
    "    print(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T16:10:04.831759Z",
     "iopub.status.busy": "2025-05-25T16:10:04.831497Z",
     "iopub.status.idle": "2025-05-25T16:14:56.961800Z",
     "shell.execute_reply": "2025-05-25T16:14:56.961000Z",
     "shell.execute_reply.started": "2025-05-25T16:10:04.831742Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1)  list of ~138k cleaned abstracts\n",
    "print(f\"Total abstracts: {len(cleaned_abstracts)}\")\n",
    "\n",
    "# 2) point here at your fine-tuned model dir\n",
    "model_dir = \"/kaggle/working/distilbert-finetuned\"\n",
    "\n",
    "# 3) load tokenizer + model (base model, so we get hidden-states)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = AutoModel.from_pretrained(model_dir)\n",
    "\n",
    "# 4) to GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_size = 64\n",
    "all_embeds = []\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(0, len(cleaned_abstracts), batch_size), desc=\"Embedding\"):\n",
    "    batch = cleaned_abstracts[i : i + batch_size]\n",
    "    \n",
    "    # tokenize + move to device\n",
    "    inputs = tokenizer(batch,\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       max_length=128).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    \n",
    "    # CLS token is at position 0\n",
    "    cls_emb = out.last_hidden_state[:, 0, :].cpu()  \n",
    "    all_embeds.append(cls_emb)\n",
    "\n",
    "# concat into one tensor\n",
    "embeddings = torch.cat(all_embeds, dim=0)  # (138000, hidden_size)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Done in {end-start:.1f}s — embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# save to disk (as NumPy .npy)\n",
    "np.save(\"cls_embeddings.npy\", embeddings.numpy())\n",
    "print(\"Saved → cls_embeddings.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T18:19:22.715Z",
     "iopub.execute_input": "2025-05-25T17:40:57.980889Z",
     "iopub.status.busy": "2025-05-25T17:40:57.980550Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = \"allenai/scibert_scivocab_uncased\"\n",
    "TRAIN_FILE,  VALID_FILE  = \"/kaggle/working/train.csv\", \"/kaggle/working/dev.csv\"\n",
    "OUTPUT_DIR    = \"./scibert-finetuned\"\n",
    "NUM_EPOCHS    = 3\n",
    "BATCH_SIZE    = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH    = 128\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"     # local cache\n",
    "os.environ[\"WANDB_DISABLED\"]    = \"true\"           # no wandb prompt\n",
    "\n",
    "# 1. Load CSVs\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": TRAIN_FILE, \"validation\": VALID_FILE},\n",
    "    cache_dir=\"./hf_cache\",\n",
    ")\n",
    "\n",
    "# 2. Tokeniser + preprocessing\n",
    "tok = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def preprocess(batch):\n",
    "    enc = tok(\n",
    "        [str(x) for x in batch[\"sentence1\"]],\n",
    "        [str(x) for x in batch[\"sentence2\"]],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    enc[\"labels\"] = [float(y) for y in batch[\"label\"]]   # regression target\n",
    "    return enc\n",
    "\n",
    "ds_tok = ds.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence1\", \"sentence2\", \"label\"],  # drop raw text/old label\n",
    ")\n",
    "\n",
    "collate = DataCollatorWithPadding(tok)\n",
    "\n",
    "# 3. Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    problem_type=\"regression\",\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "# 4. Metric (NumPy Pearson)\n",
    "def compute_metrics(pred):\n",
    "    p, l = pred\n",
    "    return {\"pearson\": float(np.corrcoef(p.flatten(), l.astype(float))[0, 1])}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    do_train=True, do_eval=True,\n",
    "    num_train_epochs=2,                 # ← one epoch first\n",
    "    per_device_train_batch_size=32,     # ← or 64 if GPU fits\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"pearson\",\n",
    "    save_total_limit=1,\n",
    "    greater_is_better=True,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=True,                          # only if GPU is on\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 6. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collate,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# 7. Train & save\n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "# LOCAL OUTPUT_DIR ———\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T22:05:25.794296Z",
     "iopub.status.busy": "2025-05-25T22:05:25.793703Z",
     "iopub.status.idle": "2025-05-25T22:28:36.205931Z",
     "shell.execute_reply": "2025-05-25T22:28:36.205104Z",
     "shell.execute_reply.started": "2025-05-25T22:05:25.794264Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── Config ─────────────────────────────────────────\n",
    "ABSTRACTS_TXT = '/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt'\n",
    "OUTPUT_PKL    = 'scibert_embeddings.pkl'\n",
    "PRETRAINED    = 'allenai/scibert_scivocab_uncased'  # or _cased\n",
    "\n",
    "# ─── 1. Load abstracts ─────────────────────────────\n",
    "abstracts = []\n",
    "with open(ABSTRACTS_TXT, 'r', encoding='utf-8') as f:\n",
    "    for ln in f:\n",
    "        parts = ln.split('|--|')\n",
    "        txt = parts[1].strip() if len(parts)==2 else ln.strip()\n",
    "        abstracts.append(txt)\n",
    "\n",
    "# ─── 2. Load SciBERT ───────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "model     = AutoModel.from_pretrained(PRETRAINED)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "# ─── 3. Compute embeddings ──────────────────────────\n",
    "batch_size = 16\n",
    "embeddings = {}  # paper_id -> vector\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(abstracts), batch_size), desc=\"Encoding abstracts\"):\n",
    "        batch_texts = abstracts[i : i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k,v in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        # take the [CLS] token representation\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        for idx, vec in enumerate(cls_emb, start=i):\n",
    "            embeddings[idx] = vec\n",
    "\n",
    "# ─── 4. Save to disk ────────────────────────────────\n",
    "with open(OUTPUT_PKL, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(f\"✅ Saved {len(embeddings)} SciBERT embeddings to {OUTPUT_PKL}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11214388,
     "sourceId": 93866,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
