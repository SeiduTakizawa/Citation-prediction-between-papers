{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":93866,"databundleVersionId":11214388,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport random\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#abstract text cleaning for upper case and delimeters\ncleaned_abstracts = []\nwith open(\"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        \n        parts = line.split(\"|--|\")\n        if len(parts) == 2:\n            abstract_text = parts[1].strip()\n        else:\n            abstract_text = line.strip() \n        # 2) Convert to lowercase if using an uncased model\n        abstract_text = abstract_text.lower()\n\n        cleaned_abstracts.append(abstract_text)\n    print(f\"Abstract {1}:\\n{abstract_text}\\n\")\n    size=len(cleaned_abstracts)\n    print(size)","metadata":{"_uuid":"f40b8c59-6c71-4126-b7fb-da8d9f072e3d","_cell_guid":"87e1edd5-7bd1-4514-854b-7efc6d62cf91","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#DISTILIBERT CLS TOKEN EXTRACTION FROM OUR FINE-TUNED MODEL\n\n#  list of ~138k cleaned abstracts\nprint(f\"Total abstracts: {len(cleaned_abstracts)}\")\n\n#  fine-tuned model dir(not in this notebook)\nmodel_dir = \"/kaggle/working/distilbert-finetuned\"\n\n#  load tokenizer + model (base model, so we get hidden-states)\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel     = AutoModel.from_pretrained(model_dir)\n\n# GPU  check for faster extraction\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nbatch_size = 64\nall_embeds = []\n\nstart = time.time()\nfor i in tqdm(range(0, len(cleaned_abstracts), batch_size), desc=\"Embedding\"):\n    batch = cleaned_abstracts[i : i + batch_size]\n    \n    # tokenize \n    inputs = tokenizer(batch,\n                       return_tensors=\"pt\",\n                       padding=True,\n                       truncation=True,\n                       max_length=128).to(device)\n    \n    with torch.no_grad():\n        out = model(**inputs)\n    \n    # CLS token is at position 0\n    cls_emb = out.last_hidden_state[:, 0, :].cpu()  \n    all_embeds.append(cls_emb)\n\n# concat into one tensor\nembeddings = torch.cat(all_embeds, dim=0)  # (138000, hidden_size)\nend = time.time()\n\nprint(f\"Done in {end-start:.1f}s â€” embeddings shape: {embeddings.shape}\")\n\n# save to disk (as NumPy .npy)\nnp.save(\"cls_embeddings.npy\", embeddings.numpy())\nprint(\"Saved  cls_embeddings.npy\")","metadata":{"_uuid":"93878cda-f012-4730-8471-92cd497545ce","_cell_guid":"5ab4626a-da9d-4afb-b1b4-8a1fd46163b3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#SCIBERT CLS TOKEN EXTRACTION FROM ABSTRACTS\n\n\n# model config\nOUTPUT_PKL = 'scibert_embeddings.pkl'\nPRETRAINED = 'allenai/scibert_scivocab_uncased' \n\n# tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\nmodel     = AutoModel.from_pretrained(PRETRAINED)\nmodel.eval()\nif torch.cuda.is_available():\n    model.to('cuda')\n\n\nbatch_size = 16\nembeddings = {}  # paper_id -> vector\n# cls token extraction\nwith torch.no_grad():\n    for i in tqdm(range(0, len(cleaned_abstracts), batch_size), desc=\"Encoding abstracts\"):\n        batch_texts = abstracts[i : i + batch_size]\n        inputs = tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n        if torch.cuda.is_available():\n            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n\n        outputs = model(**inputs)\n        cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n        for idx, vec in enumerate(cls_emb, start=i):\n            embeddings[idx] = vec\n\n# Save to disk as pkl\nwith open(OUTPUT_PKL, 'wb') as f:\n    pickle.dump(embeddings, f)\n\nprint(f\" Saved {len(embeddings)} SciBERT embeddings to {OUTPUT_PKL}\")","metadata":{"_uuid":"37cef2c9-63c1-478b-a17c-34b4d7196b73","_cell_guid":"00d11122-c357-444f-a519-610803d6838b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install node2vec","metadata":{"_uuid":"16850e95-4491-4587-b2ea-80edab90b18d","_cell_guid":"bfbb185c-32fd-4eef-b679-b16234598371","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TUNED N2V RICH EMBEDDING EXTRACTION\n\n\n# Paths & Config\n\nEDGES_POS_PATH = \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\"\nOUTPUT_EMB = \"node2vec_embeddings_train.pkl\"\nTEST_SIZE = 0.2\nRNG_SEED = 42\n\n# Limit threads for reproducibility\nos.environ[\"OMP_NUM_THREADS\"] = \"2\"\n\n\n# 1. Load and split edges safely\n\nprint(\"ðŸ“¥ Loading positive edges...\")\nedges_pos = pd.read_csv(\n    EDGES_POS_PATH,\n    header=None,\n    names=[\"u\", \"v\"],\n    dtype={\"u\": int, \"v\": int}\n)\nprint(f\"âœ… {len(edges_pos)} positive edges loaded.\")\n\n# Combine with negatives solely to stratify-split \n# Here we split only positives for embedding training to avoid data-leakage\nedges_train, edges_test = train_test_split(\n    edges_pos,\n    test_size=TEST_SIZE,\n    random_state=RNG_SEED\n)\nedges_train = edges_train.reset_index(drop=True)\nedges_test = edges_test.reset_index(drop=True)\nprint(f\"ðŸ“¤ Split into {len(edges_train)} train and {len(edges_test)} test positive edges.\")\n\n\n# Build train-only graph\n\nprint(\" Building train-only graph...\")\nG = nx.Graph()\nG.add_edges_from(edges_train.values.tolist())\n# Ensure all nodes appear (optional, for embedding coverage)\nall_nodes = set(edges_pos.u) | set(edges_pos.v)\nG.add_nodes_from(all_nodes)\nprint(f\"âœ… Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} train edges.\")\n\n\n# Train Node2Vec on train graph\n\nprint(\" Training Node2Vec embeddings...\")\nstart = time.time()\nn2v = Node2Vec(\n    graph=G,\n    dimensions=128,\n    walk_length=30,\n    num_walks=30,\n    workers=2,\n    p=1, q=1,\n    seed=RNG_SEED\n)\nmodel = n2v.fit(\n    window=5,\n    min_count=1,\n    batch_words=10000\n)\nprint(f\" Node2Vec training completed in {time.time() - start:.2f}s.\")\n\n\n# Extract and save embeddings as pkl\n\nprint(\" Extracting and saving embeddings...\")\nembeddings = {int(node): model.wv[node] for node in model.wv.index_to_key}\nwith open(OUTPUT_EMB, \"wb\") as f:\n    pickle.dump(embeddings, f)\nprint(f\" Saved embeddings for {len(embeddings)} nodes to '{OUTPUT_EMB}'.\")","metadata":{"_uuid":"a2864678-173e-42a0-96f7-d780f87693bd","_cell_guid":"4a865f5d-58fa-4cb2-b18c-906dfc1a17fb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#FEATURE EXTRACTION FROM THE CITATION GRAPH\n\n\n\n# Paths\n\nEDGES_POS_PATH = \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\"\nGRAPH_FEAT_OUT = \"graph_features.pkl\"\n\n\n#  Load positive edges\n\nedges_pos = pd.read_csv(\n    EDGES_POS_PATH,\n    header=None,\n    names=[\"u\", \"v\"],\n    dtype={\"u\": int, \"v\": int}\n)\nprint(f\"Loaded {len(edges_pos)} positive edges.\")\n\n\n#  Negative sampling (no overlap with any positives)\n\ndef sample_negatives(edges_pos: pd.DataFrame, rng_seed: int = 42) -> pd.DataFrame:\n    rng = np.random.default_rng(rng_seed)\n    pos_set = set(map(tuple, edges_pos.values))\n    max_node = int(edges_pos.values.max()) + 1\n    neg = set()\n    while len(neg) < len(edges_pos):\n        u, v = rng.integers(0, max_node, size=2)\n        if u != v and (u, v) not in pos_set and (v, u) not in pos_set:\n            neg.add((u, v))\n    return pd.DataFrame(list(neg), columns=[\"u\", \"v\"])\n\nedges_neg = sample_negatives(edges_pos)\nprint(f\"Sampled {len(edges_neg)} negative edges.\")\n\n\n# Combine & stratified train/test split\n\nedges_pos[\"y\"] = 1\nedges_neg[\"y\"] = 0\nfull_df = pd.concat([edges_pos, edges_neg], ignore_index=True)\n\ntrain_df, test_df = train_test_split(\n    full_df,\n    test_size=0.2,\n    random_state=42,\n    stratify=full_df[\"y\"]\n)\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\nprint(f\"Train pairs: {len(train_df)}, Test pairs: {len(test_df)}\")\n\n\n#  Build training graph (only train positives)\n\nG = nx.Graph()\npositives_train = train_df.loc[train_df.y == 1, [\"u\", \"v\"]].values\nG.add_edges_from(positives_train)\nprint(f\"Training graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n\n#  Define graph features (computed on G)\n\n\ndef compute_graph_features(G: nx.Graph, pairs: pd.DataFrame) -> dict:\n    feat_dict = {}\n    for u, v in tqdm(pairs[['u', 'v']].itertuples(index=False), total=len(pairs), desc=\"Computing graph features\"):\n        # Common neighbors count (log-scaled)\n        try:\n            cn = np.log1p(len(list(nx.common_neighbors(G, u, v))))\n        except:\n            cn = 0.0\n        # Preferential attachment score (log-scaled)\n        try:\n            pa_raw = next((score for (_, _, score) in nx.preferential_attachment(G, [(u, v)])), 0)\n            pa = np.log1p(pa_raw)\n        except:\n            pa = 0.0\n        feat_dict[(u, v)] = np.array([cn, pa], dtype=np.float32)\n    return feat_dict\n\n# Compute features for both train and test pairs\ngraph_feat_train = compute_graph_features(G, train_df)\ngraph_feat_test  = compute_graph_features(G, test_df)\n\n# Merge dicts\ngraph_feat_all = {**graph_feat_train, **graph_feat_test}\n\n\n#  Save graph features\n\nwith open(GRAPH_FEAT_OUT, \"wb\") as f:\n    pickle.dump(graph_feat_all, f)\nprint(f\"Saved leakage-free graph features to '{GRAPH_FEAT_OUT}'\")","metadata":{"_uuid":"0b3a4a42-b17c-4235-84cd-5e19b8983f52","_cell_guid":"df2a8b25-a248-4486-8710-4a6b09af98c5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#PLOTTING FOR GRAPH FEATURES\n\n# Load features (to get edge list)\nwith open(\"graph_features.pkl\", \"rb\") as f:\n    graph_feat_all = pickle.load(f)\n\n# Build full graph from all feature keys\nG_full = nx.Graph()\nG_full.add_edges_from(graph_feat_all.keys())\n\n# Find the largest connected component\nlargest_cc = max(nx.connected_components(G_full), key=len)\nG_lcc = G_full.subgraph(largest_cc).copy()\n\n# Sample 100 nodes from this connected component\nsample_nodes = random.sample(list(G_lcc.nodes), 1000)\n\n# Create the induced subgraph (still connected)\nG_sample = G_lcc.subgraph(sample_nodes).copy()\n\n# Layout and visualization\npos = nx.spring_layout(G_sample, seed=42)\nplt.figure(figsize=(10, 8))\nnx.draw(\n    G_sample, pos, with_labels=True, node_color=\"skyblue\",\n    edge_color=\"gray\", node_size=300, font_size=7\n)\nplt.title(\" Connected Subgraph Visualization (1000 nodes)\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"e8207557-1e96-47ff-8d72-091bff210d5f","_cell_guid":"49f60492-dd72-46c2-b3ff-3720664e478d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load  graph features dict\nwith open(\"graph_features.pkl\", \"rb\") as f:\n    graph_feat = pickle.load(f)\n\n# Stack into arrays\ndata = np.stack(list(graph_feat.values()), axis=0)\ncn = data[:, 0]   # log(common neighbors + 1)\npa = data[:, 1]   # log(preferential attachment + 1)\n\n# Compute and print summary stats\nfor name, arr in [(\"Common Neighbors\", cn), (\"Preferential Attachment\", pa)]:\n    print(f\"{name}:\")\n    print(f\"  count = {len(arr)}\")\n    print(f\"  mean  = {arr.mean():.3f}\")\n    print(f\"  median= {np.median(arr):.3f}\")\n    print(f\"  std   = {arr.std():.3f}\")\n    print()\n\n# Plot two separate histograms\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), tight_layout=True)\n\nax1.hist(cn, bins=50)\nax1.set_title(\"Distribution of log(Common Neighbors + 1)\")\nax1.set_xlabel(\"log(Common Neighbors + 1)\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(pa, bins=50)\nax2.set_title(\"Distribution of log(Preferential Attachment + 1)\")\nax2.set_xlabel(\"log(Preferential Attachment + 1)\")\nax2.set_ylabel(\"Count\")\n\nplt.show()","metadata":{"_uuid":"55ab1c5e-e7bb-4660-9b70-3aff48e8aac8","_cell_guid":"1375a128-b996-4d3c-8536-cc382f74e472","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}