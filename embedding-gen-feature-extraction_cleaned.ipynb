{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T22:54:38.979786Z",
     "iopub.status.busy": "2025-06-14T22:54:38.979478Z",
     "iopub.status.idle": "2025-06-14T22:54:38.986069Z",
     "shell.execute_reply": "2025-06-14T22:54:38.984635Z",
     "shell.execute_reply.started": "2025-06-14T22:54:38.979762Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "87e1edd5-7bd1-4514-854b-7efc6d62cf91",
    "_uuid": "f40b8c59-6c71-4126-b7fb-da8d9f072e3d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#abstract text cleaning for upper case and delimeters\n",
    "cleaned_abstracts = []\n",
    "with open(\"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        \n",
    "        parts = line.split(\"|--|\")\n",
    "        if len(parts) == 2:\n",
    "            abstract_text = parts[1].strip()\n",
    "        else:\n",
    "            abstract_text = line.strip() \n",
    "        # 2) Convert to lowercase if using an uncased model\n",
    "        abstract_text = abstract_text.lower()\n",
    "\n",
    "        cleaned_abstracts.append(abstract_text)\n",
    "    print(f\"Abstract {1}:\\n{abstract_text}\\n\")\n",
    "    size=len(cleaned_abstracts)\n",
    "    print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5ab4626a-da9d-4afb-b1b4-8a1fd46163b3",
    "_uuid": "93878cda-f012-4730-8471-92cd497545ce",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#DISTILIBERT CLS TOKEN EXTRACTION FROM OUR FINE-TUNED MODEL\n",
    "\n",
    "#  list of ~138k cleaned abstracts\n",
    "print(f\"Total abstracts: {len(cleaned_abstracts)}\")\n",
    "\n",
    "#  fine-tuned model dir(not in this notebook)\n",
    "model_dir = \"/kaggle/working/distilbert-finetuned\"\n",
    "\n",
    "#  load tokenizer + model (base model, so we get hidden-states)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model     = AutoModel.from_pretrained(model_dir)\n",
    "\n",
    "# GPU  check for faster extraction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_size = 64\n",
    "all_embeds = []\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(0, len(cleaned_abstracts), batch_size), desc=\"Embedding\"):\n",
    "    batch = cleaned_abstracts[i : i + batch_size]\n",
    "    \n",
    "    # tokenize \n",
    "    inputs = tokenizer(batch,\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       max_length=128).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    \n",
    "    # CLS token is at position 0\n",
    "    cls_emb = out.last_hidden_state[:, 0, :].cpu()  \n",
    "    all_embeds.append(cls_emb)\n",
    "\n",
    "# concat into one tensor\n",
    "embeddings = torch.cat(all_embeds, dim=0)  # (138000, hidden_size)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Done in {end-start:.1f}s â€” embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# save to disk (as NumPy .npy)\n",
    "np.save(\"cls_embeddings.npy\", embeddings.numpy())\n",
    "print(\"Saved  cls_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00d11122-c357-444f-a519-610803d6838b",
    "_uuid": "37cef2c9-63c1-478b-a17c-34b4d7196b73",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SCIBERT CLS TOKEN EXTRACTION FROM ABSTRACTS\n",
    "\n",
    "\n",
    "# model config\n",
    "OUTPUT_PKL = 'scibert_embeddings.pkl'\n",
    "PRETRAINED = 'allenai/scibert_scivocab_uncased' \n",
    "\n",
    "# tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\n",
    "model     = AutoModel.from_pretrained(PRETRAINED)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "embeddings = {}  # paper_id -> vector\n",
    "# cls token extraction\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(cleaned_abstracts), batch_size), desc=\"Encoding abstracts\"):\n",
    "        batch_texts = abstracts[i : i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        for idx, vec in enumerate(cls_emb, start=i):\n",
    "            embeddings[idx] = vec\n",
    "\n",
    "# Save to disk as pkl\n",
    "with open(OUTPUT_PKL, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(f\" Saved {len(embeddings)} SciBERT embeddings to {OUTPUT_PKL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bfbb185c-32fd-4eef-b679-b16234598371",
    "_uuid": "16850e95-4491-4587-b2ea-80edab90b18d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a865f5d-58fa-4cb2-b18c-906dfc1a17fb",
    "_uuid": "a2864678-173e-42a0-96f7-d780f87693bd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TUNED N2V RICH EMBEDDING EXTRACTION\n",
    "\n",
    "\n",
    "# Paths & Config\n",
    "\n",
    "EDGES_POS_PATH = \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\"\n",
    "OUTPUT_EMB = \"node2vec_embeddings_train.pkl\"\n",
    "TEST_SIZE = 0.2\n",
    "RNG_SEED = 42\n",
    "\n",
    "# Limit threads for reproducibility\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "\n",
    "# 1. Load and split edges safely\n",
    "\n",
    "print(\"ðŸ“¥ Loading positive edges...\")\n",
    "edges_pos = pd.read_csv(\n",
    "    EDGES_POS_PATH,\n",
    "    header=None,\n",
    "    names=[\"u\", \"v\"],\n",
    "    dtype={\"u\": int, \"v\": int}\n",
    ")\n",
    "print(f\"âœ… {len(edges_pos)} positive edges loaded.\")\n",
    "\n",
    "# Combine with negatives solely to stratify-split \n",
    "# Here we split only positives for embedding training to avoid data-leakage\n",
    "edges_train, edges_test = train_test_split(\n",
    "    edges_pos,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RNG_SEED\n",
    ")\n",
    "edges_train = edges_train.reset_index(drop=True)\n",
    "edges_test = edges_test.reset_index(drop=True)\n",
    "print(f\"ðŸ“¤ Split into {len(edges_train)} train and {len(edges_test)} test positive edges.\")\n",
    "\n",
    "\n",
    "# Build train-only graph\n",
    "\n",
    "print(\" Building train-only graph...\")\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges_train.values.tolist())\n",
    "# Ensure all nodes appear (optional, for embedding coverage)\n",
    "all_nodes = set(edges_pos.u) | set(edges_pos.v)\n",
    "G.add_nodes_from(all_nodes)\n",
    "print(f\"âœ… Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} train edges.\")\n",
    "\n",
    "\n",
    "# Train Node2Vec on train graph\n",
    "\n",
    "print(\" Training Node2Vec embeddings...\")\n",
    "start = time.time()\n",
    "n2v = Node2Vec(\n",
    "    graph=G,\n",
    "    dimensions=128,\n",
    "    walk_length=30,\n",
    "    num_walks=30,\n",
    "    workers=2,\n",
    "    p=1, q=1,\n",
    "    seed=RNG_SEED\n",
    ")\n",
    "model = n2v.fit(\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    batch_words=10000\n",
    ")\n",
    "print(f\" Node2Vec training completed in {time.time() - start:.2f}s.\")\n",
    "\n",
    "\n",
    "# Extract and save embeddings as pkl\n",
    "\n",
    "print(\" Extracting and saving embeddings...\")\n",
    "embeddings = {int(node): model.wv[node] for node in model.wv.index_to_key}\n",
    "with open(OUTPUT_EMB, \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "print(f\" Saved embeddings for {len(embeddings)} nodes to '{OUTPUT_EMB}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df2a8b25-a248-4486-8710-4a6b09af98c5",
    "_uuid": "0b3a4a42-b17c-4235-84cd-5e19b8983f52",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-14T22:54:44.414480Z",
     "iopub.status.busy": "2025-06-14T22:54:44.414132Z",
     "iopub.status.idle": "2025-06-14T22:56:27.227511Z",
     "shell.execute_reply": "2025-06-14T22:56:27.226280Z",
     "shell.execute_reply.started": "2025-06-14T22:54:44.414459Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#FEATURE EXTRACTION FROM THE CITATION GRAPH\n",
    "\n",
    "\n",
    "\n",
    "# Paths\n",
    "\n",
    "EDGES_POS_PATH = \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\"\n",
    "GRAPH_FEAT_OUT = \"graph_features.pkl\"\n",
    "\n",
    "\n",
    "#  Load positive edges\n",
    "\n",
    "edges_pos = pd.read_csv(\n",
    "    EDGES_POS_PATH,\n",
    "    header=None,\n",
    "    names=[\"u\", \"v\"],\n",
    "    dtype={\"u\": int, \"v\": int}\n",
    ")\n",
    "print(f\"Loaded {len(edges_pos)} positive edges.\")\n",
    "\n",
    "\n",
    "#  Negative sampling (no overlap with any positives)\n",
    "\n",
    "def sample_negatives(edges_pos: pd.DataFrame, rng_seed: int = 42) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    pos_set = set(map(tuple, edges_pos.values))\n",
    "    max_node = int(edges_pos.values.max()) + 1\n",
    "    neg = set()\n",
    "    while len(neg) < len(edges_pos):\n",
    "        u, v = rng.integers(0, max_node, size=2)\n",
    "        if u != v and (u, v) not in pos_set and (v, u) not in pos_set:\n",
    "            neg.add((u, v))\n",
    "    return pd.DataFrame(list(neg), columns=[\"u\", \"v\"])\n",
    "\n",
    "edges_neg = sample_negatives(edges_pos)\n",
    "print(f\"Sampled {len(edges_neg)} negative edges.\")\n",
    "\n",
    "\n",
    "# Combine & stratified train/test split\n",
    "\n",
    "edges_pos[\"y\"] = 1\n",
    "edges_neg[\"y\"] = 0\n",
    "full_df = pd.concat([edges_pos, edges_neg], ignore_index=True)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    full_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=full_df[\"y\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(f\"Train pairs: {len(train_df)}, Test pairs: {len(test_df)}\")\n",
    "\n",
    "\n",
    "#  Build training graph (only train positives)\n",
    "\n",
    "G = nx.Graph()\n",
    "positives_train = train_df.loc[train_df.y == 1, [\"u\", \"v\"]].values\n",
    "G.add_edges_from(positives_train)\n",
    "print(f\"Training graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "\n",
    "#  Define graph features (computed on G)\n",
    "\n",
    "\n",
    "def compute_graph_features(G: nx.Graph, pairs: pd.DataFrame) -> dict:\n",
    "    feat_dict = {}\n",
    "    for u, v in tqdm(pairs[['u', 'v']].itertuples(index=False), total=len(pairs), desc=\"Computing graph features\"):\n",
    "        # Common neighbors count (log-scaled)\n",
    "        try:\n",
    "            cn = np.log1p(len(list(nx.common_neighbors(G, u, v))))\n",
    "        except:\n",
    "            cn = 0.0\n",
    "        # Preferential attachment score (log-scaled)\n",
    "        try:\n",
    "            pa_raw = next((score for (_, _, score) in nx.preferential_attachment(G, [(u, v)])), 0)\n",
    "            pa = np.log1p(pa_raw)\n",
    "        except:\n",
    "            pa = 0.0\n",
    "        feat_dict[(u, v)] = np.array([cn, pa], dtype=np.float32)\n",
    "    return feat_dict\n",
    "\n",
    "# Compute features for both train and test pairs\n",
    "graph_feat_train = compute_graph_features(G, train_df)\n",
    "graph_feat_test  = compute_graph_features(G, test_df)\n",
    "\n",
    "# Merge dicts\n",
    "graph_feat_all = {**graph_feat_train, **graph_feat_test}\n",
    "\n",
    "\n",
    "#  Save graph features\n",
    "\n",
    "with open(GRAPH_FEAT_OUT, \"wb\") as f:\n",
    "    pickle.dump(graph_feat_all, f)\n",
    "print(f\"Saved leakage-free graph features to '{GRAPH_FEAT_OUT}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49f60492-dd72-46c2-b3ff-3720664e478d",
    "_uuid": "e8207557-1e96-47ff-8d72-091bff210d5f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-14T22:57:57.511205Z",
     "iopub.status.busy": "2025-06-14T22:57:57.510817Z",
     "iopub.status.idle": "2025-06-14T22:58:54.594580Z",
     "shell.execute_reply": "2025-06-14T22:58:54.593132Z",
     "shell.execute_reply.started": "2025-06-14T22:57:57.511178Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#PLOTTING FOR GRAPH FEATURES\n",
    "\n",
    "# Load features (to get edge list)\n",
    "with open(\"graph_features.pkl\", \"rb\") as f:\n",
    "    graph_feat_all = pickle.load(f)\n",
    "\n",
    "# Build full graph from all feature keys\n",
    "G_full = nx.Graph()\n",
    "G_full.add_edges_from(graph_feat_all.keys())\n",
    "\n",
    "# Find the largest connected component\n",
    "largest_cc = max(nx.connected_components(G_full), key=len)\n",
    "G_lcc = G_full.subgraph(largest_cc).copy()\n",
    "\n",
    "# Sample 100 nodes from this connected component\n",
    "sample_nodes = random.sample(list(G_lcc.nodes), 1000)\n",
    "\n",
    "# Create the induced subgraph (still connected)\n",
    "G_sample = G_lcc.subgraph(sample_nodes).copy()\n",
    "\n",
    "# Layout and visualization\n",
    "pos = nx.spring_layout(G_sample, seed=42)\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(\n",
    "    G_sample, pos, with_labels=True, node_color=\"skyblue\",\n",
    "    edge_color=\"gray\", node_size=300, font_size=7\n",
    ")\n",
    "plt.title(\" Connected Subgraph Visualization (1000 nodes)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1375a128-b996-4d3c-8536-cc382f74e472",
    "_uuid": "55ab1c5e-e7bb-4660-9b70-3aff48e8aac8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-14T22:59:11.925517Z",
     "iopub.status.busy": "2025-06-14T22:59:11.924770Z",
     "iopub.status.idle": "2025-06-14T22:59:19.392693Z",
     "shell.execute_reply": "2025-06-14T22:59:19.391153Z",
     "shell.execute_reply.started": "2025-06-14T22:59:11.925484Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load  graph features dict\n",
    "with open(\"graph_features.pkl\", \"rb\") as f:\n",
    "    graph_feat = pickle.load(f)\n",
    "\n",
    "# Stack into arrays\n",
    "data = np.stack(list(graph_feat.values()), axis=0)\n",
    "cn = data[:, 0]   # log(common neighbors + 1)\n",
    "pa = data[:, 1]   # log(preferential attachment + 1)\n",
    "\n",
    "# Compute and print summary stats\n",
    "for name, arr in [(\"Common Neighbors\", cn), (\"Preferential Attachment\", pa)]:\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  count = {len(arr)}\")\n",
    "    print(f\"  mean  = {arr.mean():.3f}\")\n",
    "    print(f\"  median= {np.median(arr):.3f}\")\n",
    "    print(f\"  std   = {arr.std():.3f}\")\n",
    "    print()\n",
    "\n",
    "# Plot two separate histograms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), tight_layout=True)\n",
    "\n",
    "ax1.hist(cn, bins=50)\n",
    "ax1.set_title(\"Distribution of log(Common Neighbors + 1)\")\n",
    "ax1.set_xlabel(\"log(Common Neighbors + 1)\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "\n",
    "ax2.hist(pa, bins=50)\n",
    "ax2.set_title(\"Distribution of log(Preferential Attachment + 1)\")\n",
    "ax2.set_xlabel(\"log(Preferential Attachment + 1)\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11214388,
     "sourceId": 93866,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
