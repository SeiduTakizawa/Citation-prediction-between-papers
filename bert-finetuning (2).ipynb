{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93866,"databundleVersionId":11214388,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\n\n# Paths \nabstracts_path = \"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\"\nedgelist_path =  \"/kaggle/input/nlp-cse-uoi-2025/data_new/edgelist.txt\"\n\n# 1. Load abstracts\nabstracts = {}\nwith open(abstracts_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        pid, txt = line.strip().split(\"|--|\", maxsplit=1)\n        abstracts[int(pid)] = txt.strip()\n\n# 2. Build positive pairs\npos = []\npos_set = set()\nwith open(edgelist_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        a, b = map(int, line.strip().split(\",\"))\n        pos.append({\n            \"sentence1\": abstracts[a],\n            \"sentence2\": abstracts[b],\n            \"label\": 1\n        })\n        pos_set.add((a, b))\n\ndf_pos = pd.DataFrame(pos)\n\n# 3. Generate negative pairs (same count as positives)\nall_ids = list(abstracts.keys())\nneg = set()\nwhile len(neg) < len(df_pos):\n    a, b = random.sample(all_ids, 2)\n    if (a, b) not in pos_set and (b,a) not in pos_set:\n        neg.add((a, b))\n\ndf_neg = pd.DataFrame([\n    {\"sentence1\": abstracts[a], \"sentence2\": abstracts[b], \"label\": 0}\n    for a, b in neg\n])\n\n# 4. Combine & shuffle\ndf = pd.concat([df_pos, df_neg], ignore_index=True)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# 5. Train/dev split (85/15, stratified by label)\ntrain_df, dev_df = train_test_split(\n    df,\n    test_size=0.15,\n    random_state=42,\n    stratify=df[\"label\"]\n)\n\n# 6. Save CSVs\ntrain_df.to_csv(\"train.csv\", index=False)\ndev_df.to_csv( \"dev.csv\",   index=False)\n\nprint(f\"Train size: {len(train_df)}, Dev size: {len(dev_df)}\")\nprint(\"\\nSample from train.csv:\")\nprint(train_df.head())\nprint(\"\\nSample from dev.csv:\")\nprint(dev_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:39:03.875905Z","iopub.execute_input":"2025-05-25T11:39:03.876691Z","iopub.status.idle":"2025-05-25T11:41:17.202564Z","shell.execute_reply.started":"2025-05-25T11:39:03.876660Z","shell.execute_reply":"2025-05-25T11:41:17.201658Z"}},"outputs":[{"name":"stdout","text":"Train size: 1856323, Dev size: 327587\n\nSample from train.csv:\n                                                 sentence1  \\\n807839   View synthesis is a process for generating nov...   \n1531135  Extensional (table) constraints are an importa...   \n1304282  We present a simple variant of the k-d tree wh...   \n656790   This paper presents a passive depth map comput...   \n445727   We treat the text summarization problem as max...   \n\n                                                 sentence2  label  \n807839   The reduction of inherent ambiguities in struc...      0  \n1531135  Special-purpose constraint propagation algorit...      1  \n1304282  Estimating the age of a human from the capture...      1  \n656790   This paper presents a comparison of six machin...      0  \n445727                                                          1  \n\nSample from dev.csv:\n                                                 sentence1  \\\n785885   We present an algorithm for plane-based self-c...   \n680598   Although ‘tracking-by-detection’ is a popular ...   \n1834102  Point set registration is a key component in m...   \n1752449  We propose a learning-based framework for acqu...   \n857050   Superpixels are perceptually meaningful atomic...   \n\n                                                 sentence2  label  \n785885   We present a technique to linearly estimate th...      1  \n680598   When clustering a dataset, the right number k ...      0  \n1834102  We propose a systematic approach for registeri...      1  \n1752449  A radial basis function (RBF) neural network a...      0  \n857050   We present an improved version of the Simple L...      1  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n)\n\n\nPRETRAINED_MODEL = \"distilbert-base-uncased\"\nTRAIN_FILE,  VALID_FILE  = \"/kaggle/working/train.csv\", \"/kaggle/working/dev.csv\"\nOUTPUT_DIR    = \"./distilbert-finetuned\"\nNUM_EPOCHS    = 3\nBATCH_SIZE    = 16\nLEARNING_RATE = 2e-5\nMAX_LENGTH    = 128\nos.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"     # local cache\nos.environ[\"WANDB_DISABLED\"]    = \"true\"           # no wandb prompt\n\n\n# 1. Load CSVs\nds = load_dataset(\n    \"csv\",\n    data_files={\"train\": TRAIN_FILE, \"validation\": VALID_FILE},\n    cache_dir=\"./hf_cache\",\n)\n\n# 2. Tokeniser + preprocessing\ntok = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n\ndef preprocess(batch):\n    enc = tok(\n        [str(x) for x in batch[\"sentence1\"]],\n        [str(x) for x in batch[\"sentence2\"]],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_LENGTH,\n    )\n    enc[\"labels\"] = [float(y) for y in batch[\"label\"]]   # regression target\n    return enc\n\nds_tok = ds.map(\n    preprocess,\n    batched=True,\n    remove_columns=[\"sentence1\", \"sentence2\", \"label\"],  # drop raw text/old label\n)\n\ncollate = DataCollatorWithPadding(tok)\n\n# 3. Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    PRETRAINED_MODEL,\n    problem_type=\"regression\",\n    num_labels=1,\n)\n\n# 4. Metric (NumPy Pearson)\ndef compute_metrics(pred):\n    p, l = pred\n    return {\"pearson\": float(np.corrcoef(p.flatten(), l.astype(float))[0, 1])}\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    do_train=True, do_eval=True,\n    num_train_epochs=1,                \n    per_device_train_batch_size=32,    \n    per_device_eval_batch_size=64,\n    learning_rate=2e-5,\n    save_strategy=\"steps\",\n    save_steps=10000,\n    eval_strategy=\"steps\",\n    eval_steps=10000,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"pearson\",\n    save_total_limit=1,\n    greater_is_better=True,\n    dataloader_num_workers=4,\n    fp16=True,                         \n)\n\n\n\n# 6. Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_tok[\"train\"],\n    eval_dataset=ds_tok[\"validation\"],\n    tokenizer=tok,\n    data_collator=collate,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n# 7. Train & save\ntrainer.train()\nmodel.save_pretrained(OUTPUT_DIR)\ntok.save_pretrained(OUTPUT_DIR)\n# ——— LOCAL OUTPUT_DIR ———\nmodel.save_pretrained(OUTPUT_DIR)\ntok.save_pretrained(OUTPUT_DIR)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:41:17.203701Z","iopub.execute_input":"2025-05-25T11:41:17.203918Z","iopub.status.idle":"2025-05-25T15:59:42.657861Z","shell.execute_reply.started":"2025-05-25T11:41:17.203901Z","shell.execute_reply":"2025-05-25T15:59:42.656607Z"}},"outputs":[{"name":"stderr","text":"2025-05-25 11:41:29.202428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748173289.379741      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748173289.435303      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4105fae95abc491da2173d507ae135f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8957376ddf5c4260bd5f2ec294e6bc17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fd4d361d1fb4a9ab9637d9336f71c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e4ef8e649f43e6a021b832c404e6d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a14959f28034447a223ce4d4c90cfbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"374d2f351b9c4e888c49170c239d42be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1856323 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2bbd84dbac4d9b97f0e3c7e09efbde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/327587 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033d22fee6114a92bfc09e4faf98d417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b57761d29a4385a26961a5768e2ea0"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_35/1517271459.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='58011' max='58011' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [58011/58011 3:38:22, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Pearson</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10000</td>\n      <td>0.089000</td>\n      <td>0.086173</td>\n      <td>0.811838</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.075900</td>\n      <td>0.076359</td>\n      <td>0.834290</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.074600</td>\n      <td>0.070079</td>\n      <td>0.848589</td>\n    </tr>\n    <tr>\n      <td>40000</td>\n      <td>0.072000</td>\n      <td>0.067341</td>\n      <td>0.855474</td>\n    </tr>\n    <tr>\n      <td>50000</td>\n      <td>0.067800</td>\n      <td>0.064604</td>\n      <td>0.861835</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:920: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1517271459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# 3. Ωθήστε (push) το μοντέλο και τον tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_memory_footprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_buffers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0morganization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprecated_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"organization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         repo_id = self._create_repo(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_create_repo\u001b[0;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[1;32m    764\u001b[0m                 \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{organization}/{repo_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'takizawa143/fine-tuned-bert/distilbert-finetuned'. Use `repo_type` argument if needed."],"ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'takizawa143/fine-tuned-bert/distilbert-finetuned'. Use `repo_type` argument if needed.","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"#abstract text cleaning for upper case and delimeters\ncleaned_abstracts = []\nwith open(\"/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt\", \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n       \n        parts = line.split(\"|--|\")\n        if len(parts) == 2:\n            abstract_text = parts[1].strip()\n        else:\n            abstract_text = line.strip() \n        # 2) Convert to lowercase if using an uncased model\n        abstract_text = abstract_text.lower()\n\n        cleaned_abstracts.append(abstract_text)\n    print(f\"Abstract {1}:\\n{abstract_text}\\n\")\n    size=len(cleaned_abstracts)\n    print(size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:09:59.827881Z","iopub.execute_input":"2025-05-25T16:09:59.828251Z","iopub.status.idle":"2025-05-25T16:10:01.773141Z","shell.execute_reply.started":"2025-05-25T16:09:59.828216Z","shell.execute_reply":"2025-05-25T16:10:01.772305Z"}},"outputs":[{"name":"stdout","text":"Abstract 1:\nin this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. we formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current euclidean approaches, deploys alignment along geodesics. our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. we provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.\n\n138499\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import time\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\n\n# 1)  list of ~138k cleaned abstracts\nprint(f\"Total abstracts: {len(cleaned_abstracts)}\")\n\n# 2) point here at your fine-tuned model dir\nmodel_dir = \"/kaggle/working/distilbert-finetuned\"\n\n# 3) load tokenizer + model (base model, so we get hidden-states)\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel     = AutoModel.from_pretrained(model_dir)\n\n# 4) to GPU \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nbatch_size = 64\nall_embeds = []\n\nstart = time.time()\nfor i in tqdm(range(0, len(cleaned_abstracts), batch_size), desc=\"Embedding\"):\n    batch = cleaned_abstracts[i : i + batch_size]\n    \n    # tokenize + move to device\n    inputs = tokenizer(batch,\n                       return_tensors=\"pt\",\n                       padding=True,\n                       truncation=True,\n                       max_length=128).to(device)\n    \n    with torch.no_grad():\n        out = model(**inputs)\n    \n    # CLS token is at position 0\n    cls_emb = out.last_hidden_state[:, 0, :].cpu()  \n    all_embeds.append(cls_emb)\n\n# concat into one tensor\nembeddings = torch.cat(all_embeds, dim=0)  # (138000, hidden_size)\nend = time.time()\n\nprint(f\"Done in {end-start:.1f}s — embeddings shape: {embeddings.shape}\")\n\n# save to disk (as NumPy .npy)\nnp.save(\"cls_embeddings.npy\", embeddings.numpy())\nprint(\"Saved → cls_embeddings.npy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:10:04.831497Z","iopub.execute_input":"2025-05-25T16:10:04.831759Z","iopub.status.idle":"2025-05-25T16:14:56.961800Z","shell.execute_reply.started":"2025-05-25T16:10:04.831742Z","shell.execute_reply":"2025-05-25T16:14:56.961000Z"}},"outputs":[{"name":"stdout","text":"Total abstracts: 138499\n","output_type":"stream"},{"name":"stderr","text":"Embedding: 100%|██████████| 2165/2165 [04:48<00:00,  7.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done in 288.6s — embeddings shape: torch.Size([138499, 768])\nSaved → cls_embeddings.npy\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os, numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n)\n\n\nPRETRAINED_MODEL = \"allenai/scibert_scivocab_uncased\"\nTRAIN_FILE,  VALID_FILE  = \"/kaggle/working/train.csv\", \"/kaggle/working/dev.csv\"\nOUTPUT_DIR    = \"./scibert-finetuned\"\nNUM_EPOCHS    = 3\nBATCH_SIZE    = 16\nLEARNING_RATE = 2e-5\nMAX_LENGTH    = 128\nos.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"     # local cache\nos.environ[\"WANDB_DISABLED\"]    = \"true\"           # no wandb prompt\n\n# 1. Load CSVs\nds = load_dataset(\n    \"csv\",\n    data_files={\"train\": TRAIN_FILE, \"validation\": VALID_FILE},\n    cache_dir=\"./hf_cache\",\n)\n\n# 2. Tokeniser + preprocessing\ntok = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n\ndef preprocess(batch):\n    enc = tok(\n        [str(x) for x in batch[\"sentence1\"]],\n        [str(x) for x in batch[\"sentence2\"]],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_LENGTH,\n    )\n    enc[\"labels\"] = [float(y) for y in batch[\"label\"]]   # regression target\n    return enc\n\nds_tok = ds.map(\n    preprocess,\n    batched=True,\n    remove_columns=[\"sentence1\", \"sentence2\", \"label\"],  # drop raw text/old label\n)\n\ncollate = DataCollatorWithPadding(tok)\n\n# 3. Model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    PRETRAINED_MODEL,\n    problem_type=\"regression\",\n    num_labels=1,\n)\n\n# 4. Metric (NumPy Pearson)\ndef compute_metrics(pred):\n    p, l = pred\n    return {\"pearson\": float(np.corrcoef(p.flatten(), l.astype(float))[0, 1])}\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    do_train=True, do_eval=True,\n    num_train_epochs=2,                 # ← one epoch first\n    per_device_train_batch_size=32,     # ← or 64 if GPU fits\n    per_device_eval_batch_size=64,\n    learning_rate=2e-5,\n    save_strategy=\"steps\",\n    save_steps=10000,\n    eval_strategy=\"steps\",\n    eval_steps=10000,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"pearson\",\n    save_total_limit=1,\n    greater_is_better=True,\n    dataloader_num_workers=4,\n    fp16=True,                          # only if GPU is on\n)\n\n\n\n# 6. Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_tok[\"train\"],\n    eval_dataset=ds_tok[\"validation\"],\n    tokenizer=tok,\n    data_collator=collate,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n# 7. Train & save\ntrainer.train()\nmodel.save_pretrained(OUTPUT_DIR)\ntok.save_pretrained(OUTPUT_DIR)\n# LOCAL OUTPUT_DIR ———\nmodel.save_pretrained(OUTPUT_DIR)\ntok.save_pretrained(OUTPUT_DIR)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:40:57.980550Z","iopub.execute_input":"2025-05-25T17:40:57.980889Z","execution_failed":"2025-05-25T18:19:22.715Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edec2475a891469ca742a8927bceee8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc45ecffdaa4113b354c9a542784347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1856323 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec6f2377f834ebba457ae0f63972707"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/327587 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91dc915100904439a993b4e830dab17d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0139f5c6506a4ff188028e9db256eda9"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_35/2737780243.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='194' max='116022' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   194/116022 01:06 < 11:13:33, 2.87 it/s, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3313f3e2242d4c09912bef9311da885d"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom tqdm.auto import tqdm\n\n# ─── Config ─────────────────────────────────────────\nABSTRACTS_TXT = '/kaggle/input/nlp-cse-uoi-2025/data_new/abstracts.txt'\nOUTPUT_PKL    = 'scibert_embeddings.pkl'\nPRETRAINED    = 'allenai/scibert_scivocab_uncased'  # or _cased\n\n# ─── 1. Load abstracts ─────────────────────────────\nabstracts = []\nwith open(ABSTRACTS_TXT, 'r', encoding='utf-8') as f:\n    for ln in f:\n        parts = ln.split('|--|')\n        txt = parts[1].strip() if len(parts)==2 else ln.strip()\n        abstracts.append(txt)\n\n# ─── 2. Load SciBERT ───────────────────────────────\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED)\nmodel     = AutoModel.from_pretrained(PRETRAINED)\nmodel.eval()\nif torch.cuda.is_available():\n    model.to('cuda')\n\n# ─── 3. Compute embeddings ──────────────────────────\nbatch_size = 16\nembeddings = {}  # paper_id -> vector\n\nwith torch.no_grad():\n    for i in tqdm(range(0, len(abstracts), batch_size), desc=\"Encoding abstracts\"):\n        batch_texts = abstracts[i : i + batch_size]\n        inputs = tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors='pt'\n        )\n        if torch.cuda.is_available():\n            inputs = {k: v.to('cuda') for k,v in inputs.items()}\n\n        outputs = model(**inputs)\n        # take the [CLS] token representation\n        cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n        for idx, vec in enumerate(cls_emb, start=i):\n            embeddings[idx] = vec\n\n# ─── 4. Save to disk ────────────────────────────────\nwith open(OUTPUT_PKL, 'wb') as f:\n    pickle.dump(embeddings, f)\n\nprint(f\"✅ Saved {len(embeddings)} SciBERT embeddings to {OUTPUT_PKL}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T22:05:25.793703Z","iopub.execute_input":"2025-05-25T22:05:25.794296Z","iopub.status.idle":"2025-05-25T22:28:36.205931Z","shell.execute_reply.started":"2025-05-25T22:05:25.794264Z","shell.execute_reply":"2025-05-25T22:28:36.205104Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc46bd32d89482e8bc725f44c8ad2e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66412a2782554b7ca7e5d629f8ad662b"}},"metadata":{}},{"name":"stderr","text":"2025-05-25 22:05:46.747183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748210746.937994      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748210746.993709      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a448d7c926454aa7e20a3153822527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6656d4fac07e462083e0b24c2e93d81d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Encoding abstracts:   0%|          | 0/8657 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c016d8c955074c82a29cd20f91bbc929"}},"metadata":{}},{"name":"stdout","text":"✅ Saved 138499 SciBERT embeddings to scibert_embeddings.pkl\n","output_type":"stream"}],"execution_count":1}]}